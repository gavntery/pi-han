<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
   <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
   <title>PixInsight Reference Documentation | ImageIntegration</title>
   <meta name="keywords" content="image integration, image combination, average combination, median combination, image normalization, image weighting, pixel rejection, noise estimators, scale estimators, robust statistics" />
   <meta name="author" content="Juan Conejero, PTeam" />
   <meta name="description" content="Combines images pixel-by-pixel for signal-to-noise ratio improvement and rejection of spurious image structures." />
   <meta name="robots" content="INDEX,FOLLOW" />
   <meta name="generator" content="PixInsight Documentation Compiler script version 1.6.3" />
   <script type="text/javascript" src="../../pidoc/scripts/pidoc-utility.js"></script>
   <link type="text/css" href="../../pidoc/css/pidoc-common.css" rel="stylesheet" />
   <link type="text/css" href="../../pidoc/css/pidoc-highlight.css" rel="stylesheet" />
   <link type="text/css" href="../../pidoc/css/pidoc-tool.css" rel="stylesheet" />
   <link rel="icon" href="../../pidoc/icons/pidoc-icon.png" type="image/png" />
   <link rel="shortcut icon" href="../../pidoc/icons/pidoc-icon.png" type="image/png" />
</head>
<body>
<script type="text/javascript">
   pidoc_generateDynamicContents();
</script>

<h1>ImageIntegration</h1>

<hr class="separator"/>

<div id="brief">
<p>Combines images pixel-by-pixel for signal-to-noise ratio improvement and rejection of spurious image structures. <a href="#__contents__">[more]</a></p></div>

<div id="categories">
<p><strong>Categories:</strong> ImageIntegration, Preprocessing</p>
</div>

<div id="keywords">
<p><strong>Keywords:</strong> image integration, image combination, average combination, median combination, image normalization, image weighting, pixel rejection, noise estimators, scale estimators, robust statistics</p>
</div>

<h3 class="pidoc_sectionTitle" id="__toc__">Contents</h3>
<p class="pidoc_sectionToggleButton" onclick="pidoc_toggleSection( 'toc', this );">[hide]</p>
<div id="toc">
<ul>
<li class="pidoc_tocItem"><a href="#__Description__">1&emsp;Description</a>
<ul>
<li class="pidoc_tocSubitem"><a href="#__Description_:_Image_Combination__">1.1&emsp;Image Combination</a></li>
<li class="pidoc_tocSubitem"><a href="#__Description_:_Image_Weighting__">1.2&emsp;Image Weighting</a></li>
<li class="pidoc_tocSubitem"><a href="#__Description_:_Pixel_Rejection__">1.3&emsp;Pixel Rejection</a></li>
<li class="pidoc_tocSubitem"><a href="#__Description_:_Image_Normalization__">1.4&emsp;Image Normalization</a>
<ul>
<li class="pidoc_tocSubitem"><a href="#__Description_:_Image_Normalization_:_Scale_and_Location_Estimators__">1.4.1&emsp;Scale and Location Estimators</a></li>
<li class="pidoc_tocSubitem"><a href="#__Description_:_Image_Normalization_:_Rejection_Normalization__">1.4.2&emsp;Rejection Normalization</a></li>
<li class="pidoc_tocSubitem"><a href="#__Description_:_Image_Normalization_:_Output_Normalization__">1.4.3&emsp;Output Normalization</a></li>
<li class="pidoc_tocSubitem"><a href="#__Description_:_Image_Normalization_:_Recommended_Normalization_Methods__">1.4.4&emsp;Recommended Normalization Methods</a></li>
</ul>
</li>
<li class="pidoc_tocSubitem"><a href="#__Description_:_Quality_Assessment__">1.5&emsp;Quality Assessment</a></li>
</ul>
</li>
<li class="pidoc_tocItem"><a href="#__Usage__">2&emsp;Usage</a>
<ul>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Input_Images__">2.1&emsp;Input Images</a></li>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Format_Hints__">2.2&emsp;Format Hints</a></li>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Image_Integration__">2.3&emsp;Image Integration</a></li>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Pixel_Rejection_1__">2.4&emsp;Pixel Rejection (1)</a></li>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Pixel_Rejection_2__">2.5&emsp;Pixel Rejection (2)</a></li>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Pixel_Rejection_3__">2.6&emsp;Pixel Rejection (3)</a></li>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Region_of_Interest__">2.7&emsp;Region of Interest</a></li>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Rejection_Maps__">2.8&emsp;Rejection Maps</a></li>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Console_Statistics__">2.9&emsp;Console Statistics</a>
<ul>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Console_Statistics_:_Information_About_Input_Images__">2.9.1&emsp;Information About Input Images</a></li>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Console_Statistics_:_Information_About_the_Integration_Process__">2.9.2&emsp;Information About the Integration Process</a></li>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Console_Statistics_:_Pixel_Rejection_Counts__">2.9.3&emsp;Pixel Rejection Counts</a></li>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Console_Statistics_:_Noise_Evaluation_Statistics__">2.9.4&emsp;Noise Evaluation Statistics</a></li>
</ul>
</li>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Cache_Management__">2.10&emsp;Cache Management</a></li>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Scripting_and_Automation__">2.11&emsp;Scripting and Automation</a></li>
<li class="pidoc_tocSubitem"><a href="#__Usage_:_Usage_Hints__">2.12&emsp;Usage Hints</a></li>
</ul>
</li>
<li class="pidoc_tocItem"><a href="#__references__">References</a></li>
<li class="pidoc_tocItem"><a href="#__relatedTools__">Related Tools</a></li>
<li class="pidoc_tocItem"><a href="#__relatedScripts__">Related Scripts</a></li>
</ul>
</div>

<div id="__contents__">

<div class="pidoc_section" id="__Description__">
   <h3 class="pidoc_sectionTitle">1&emsp;Description</h3>
   <p class="pidoc_sectionToggleButton" onclick="pidoc_toggleSection( 'Description', this );">[hide]</p>
   <div id="Description">
<img style="float:left;margin-right:1.25em;margin-bottom:0.5em;" src="images/ImageIntegration.png" alt=""/>
<p>Repeating measurements is a fundamental technique to analyze and reduce uncertainties in observations. By combining several measurements of the same quantity, random errors tend to cancel out and the observed value can thus be determined with less uncertainty. The image integration task does basically the same thing: combine a set of images of the same subject to improve the signal-to-noise ratio in the resulting image. The ImageIntegration tool allows you to perform this task with a large number of features and resources designed to help you get the best possible result out of your data. These resources include:</p>

<ul class="pidoc_list">
<li><strong>Four pixel combination operations:</strong> mean, median, maximum and minimum.</li>
<li class="pidoc_spaced_list_item"><strong>Multiscale noise evaluation</strong> for automatic image weighting and quality assessment.</li>
<li class="pidoc_spaced_list_item"><strong>Additive and multiplicative image normalization</strong> with optional scaling and seven user-selectable robust scale estimators.</li>
<li class="pidoc_spaced_list_item"><strong>Seven pixel rejection algorithms:</strong> min/max, percentile clipping, sigma clipping, Winsorized sigma clipping, averaged (Poisson based) sigma clipping, linear fit clipping and CCD noise model rejection.</li>
<li class="pidoc_spaced_list_item"><strong>Asymmetric pixel rejection</strong> where rejection limits can be defined independently for low and high pixel values.</li>
<li class="pidoc_spaced_list_item"><strong>Range rejection</strong> to exclude too dark and saturated pixels.</li>
<li class="pidoc_spaced_list_item"><strong>Separate normalization</strong> for the pixel rejection and pixel combination processes.</li>
<li class="pidoc_spaced_list_item"><strong>Pixel rejection maps</strong> automatically generated to facilitate evaluation of rejection parameters.</li>
<li class="pidoc_spaced_list_item"><strong>Slope maps</strong> that characterize the magnitude and spatial distribution of brightness variations in the integrated data set.</li>
<li class="pidoc_spaced_list_item"><strong>Region of interest feature</strong> to speed up the process of testing integration and rejection parameters.</li>
<li class="pidoc_spaced_list_item"><strong>Built-in file cache system</strong> for fast retrieval of image statistical data.</li>
<li class="pidoc_spaced_list_item"><strong>Multithreaded execution</strong> with optimized processor scalability.</li>
</ul>

<p><br class="pidoc_clearfix"/></p>
<p><a id="image_combination"></a></p>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>
<p><a id="image_weighting"></a></p>
<p><a id="pixel_rejection"></a></p>
<p><a id="image_normalization"></a></p>
<p><a id="quality_assessment"></a></p>
<div class="pidoc_subsection" id="__Description_:_Image_Combination__">
   <h4 class="pidoc_subsectionTitle">1.1&emsp;Image Combination</h4>
<p>In the image integration task we have a set <img style="vertical-align:middle;" src="images/eqn_0001.svg" alt=""/> of images of the same region of the sky, which we have previously co-registered (for example, with the <a href="../../tools/StarAlignment/StarAlignment.html" title="../../tools/StarAlignment/StarAlignment.html">StarAlignment</a> tool). Each vector formed with the <img style="vertical-align:middle;" src="images/eqn_0002.svg" alt=""/> pixels at the same <img style="vertical-align:middle;" src="images/eqn_0003.svg" alt=""/> image coordinates,</p>
<a id="__equation_1__"></a><div class="pidoc_equation"><img src="images/eqn_0004.svg" alt=""/><span class="pidoc_equation_number">[1]</span></div>
<p>is what we call a <em>pixel stack</em>. Since the images are mutually registered, each component of a pixel stack is an observation of the same pixel in the integrated image. Assuming that all components are <a href="http://en.wikipedia.org/wiki/Independence_%28probability_theory%29" title="http://en.wikipedia.org/wiki/Independence_%28probability_theory%29">independent measures</a> of a <a href="http://en.wikipedia.org/wiki/Random_variable" title="http://en.wikipedia.org/wiki/Random_variable">random variable</a>, we can rewrite a pixel stack as</p>
<a id="__equation_2__"></a><div class="pidoc_equation"><img src="images/eqn_0005.svg" alt=""/><span class="pidoc_equation_number">[2]</span></div>
<p>where we have split each image <img style="vertical-align:middle;" src="images/eqn_0006.svg" alt=""/> into the deterministic signal of interest <img style="vertical-align:middle;" src="images/eqn_0007.svg" alt=""/> and an additive noise term <img style="vertical-align:middle;" src="images/eqn_0008.svg" alt=""/>. In the above equation, we assume that the <img style="vertical-align:middle;" src="images/eqn_0002.svg" alt=""/> noise terms form a set of <em>zero-mean random errors:</em></p>
<a id="__equation_3__"></a><div class="pidoc_equation"><img src="images/eqn_0009.svg" alt=""/><span class="pidoc_equation_number">[3]</span></div>
<p>The noise materializes the uncertainty of the acquired data, which is inherent in any observational process. The equation above tells us that uncertainty cannot be removed from the data; we can reduce it by combining images, but there is no way to suppress it completely. In the image integration problem, we are representing uncertainty as the random errors in each pixel stack. There are many sources of random errors, such as instrumental limitations, unpredictable fluctuations and reading errors. The data also come altered by processes that are not representable as random variations at the pixel level, such as spurious data (cosmic ray impacts, plane trails), light pollution gradients and the effects of limited seeing. These problems require special procedures to be dealt with, such as <a href="#pixel_rejection">pixel rejection</a> and specific post-processing techniques.</p>
<p>The image integration process combines the components of each pixel stack into one pixel of an <em>integrated image</em>. Since our main goal is to improve the <a href="http://en.wikipedia.org/wiki/Signal-to-noise_ratio" title="http://en.wikipedia.org/wiki/Signal-to-noise_ratio">signal-to-noise ratio</a> (SNR) in the integrated result, we are interested in knowing the SNR increments that can be achieved with different pixel combination operations. The two operations relevant for this subject in the ImageIntegration tool are the mean (or average) and the median, which we'll discuss below.</p>

<dl class="pidoc_list">
<dt>
<p><a id="average_combination"></a> Average Combination</p>
</dt>
<dd>
<p>When the average combination operation is applied, each pixel in the integrated image is calculated as the arithmetic mean of the components of the corresponding pixel stack, which we represent as <img style="vertical-align:middle;" src="images/eqn_0010.svg" alt=""/>:</p>
<a id="__equation_4__"></a><div class="pidoc_equation"><img src="images/eqn_0011.svg" alt=""/><span class="pidoc_equation_number">[4]</span></div>
<p>where we symbolize the average-integrated image as <img style="vertical-align:middle;" src="images/eqn_0012.svg" alt=""/>. If <img style="vertical-align:middle;" src="images/eqn_0013.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0014.svg" alt=""/> are two <a href="http://en.wikipedia.org/wiki/Uncorrelated" title="http://en.wikipedia.org/wiki/Uncorrelated">uncorrelated random variables</a>, a basic property of the <a href="http://en.wikipedia.org/wiki/Variance" title="http://en.wikipedia.org/wiki/Variance">variance</a> is</p>
<div class="pidoc_equation"><img src="images/eqn_0015.svg" alt=""/></div>
<p>The <a href="http://en.wikipedia.org/wiki/Standard_error" title="http://en.wikipedia.org/wiki/Standard_error">standard error</a> of the mean can be easily derived from this property. Suppose we have a set <img style="vertical-align:middle;" src="images/eqn_0016.svg" alt=""/> of <a href="http://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" title="http://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">independent and identically distributed</a> (iid) random variables. Since the variables are iid, all of them must have the same variance:</p>
<div class="pidoc_equation"><img src="images/eqn_0017.svg" alt=""/></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0018.svg" alt=""/> is the <a href="http://en.wikipedia.org/wiki/Standard_deviation" title="http://en.wikipedia.org/wiki/Standard_deviation">standard deviation</a> of the set. Therefore, the variance of the sum is</p>
<div class="pidoc_equation"><img src="images/eqn_0019.svg" alt=""/></div>
<p>and the variance of the mean is given by</p>
<div class="pidoc_equation"><img src="images/eqn_0020.svg" alt=""/></div>
<p>Recall that we are working with co-registered images, so the assumption that all the components of any pixel stack are iid random variables looks plausible. Now we can deduce the standard deviation of the average-integrated image from the above equation. Since the operation is performed for all pixel stacks, assuming that all of the input images are <a href="#image_normalization">normalized</a> to equal variances,</p>
<a id="__equation_5__"></a><div class="pidoc_equation"><img src="images/eqn_0021.svg" alt=""/><span class="pidoc_equation_number">[5]</span></div>
<p>The increase in signal-to-noise ratio is proportional, assuming that all of the input images have equal SNR, to the square root of the number of integrated images:</p>
<a id="__equation_6__"></a><div class="pidoc_equation"><img src="images/eqn_0022.svg" alt=""/><span class="pidoc_equation_number">[6]</span></div>
<p>Note that the mean and the sum of a set of images are strictly equivalent in terms of SNR improvement. This becomes evident from Equation <a href="#__equation_3__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 3]<br/>
<img src='images/eqn_0009.svg' alt=''/>">[3]</a>: the sum of zero-mean random errors always tends to zero, and dividing it by a constant does not change anything. The mean is used in most implementations because it prevents saturation of pixels due to accumulated out-of-range values that cannot be represented within the available numeric range.</p>
</dd>
<dt>
<p><a id="median_combination"></a> Median Combination</p>
</dt>
<dd>
<p>The median of a <a href="http://en.wikipedia.org/wiki/Probability_distribution" title="http://en.wikipedia.org/wiki/Probability_distribution">probability distribution</a> with <a href="http://en.wikipedia.org/wiki/Probability_density_function" title="http://en.wikipedia.org/wiki/Probability_density_function">density function</a> <img style="vertical-align:middle;" src="images/eqn_0023.svg" alt=""/> is the value <img style="vertical-align:middle;" src="images/eqn_0024.svg" alt=""/> for which smaller and greater values are equally probable:</p>
<div class="pidoc_equation"><img src="images/eqn_0025.svg" alt=""/></div>
<p>The <em>k</em>th <a href="http://en.wikipedia.org/wiki/Order_statistic" title="http://en.wikipedia.org/wiki/Order_statistic">order statistic</a> of a <a href="http://en.wikipedia.org/wiki/Statistical_sample" title="http://en.wikipedia.org/wiki/Statistical_sample">statistical sample</a> is defined as its <em>k</em>th smallest sample value. For example, the minimum and maximum of a sample of size <em>N</em> are its 1st and <em>N</em>th order statistics, respectively. For a distribution sample <img style="vertical-align:middle;" src="images/eqn_0001.svg" alt=""/> of odd length <img style="vertical-align:middle;" src="images/eqn_0026.svg" alt=""/> the median is the <img style="vertical-align:middle;" src="images/eqn_0027.svg" alt=""/> order statistic. For even <img style="vertical-align:middle;" src="images/eqn_0026.svg" alt=""/> the median is defined conventionally as the mean of the two central smallest elements at <img style="vertical-align:middle;" src="images/eqn_0028.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0029.svg" alt=""/>.</p>
<p>When the median combination operation is selected, the value of each pixel in the integrated image is the median of the components of the corresponding pixel stack:</p>
<a id="__equation_7__"></a><div class="pidoc_equation"><img src="images/eqn_0030.svg" alt=""/><span class="pidoc_equation_number">[7]</span></div>
<p>where we represent the median-integrated image as <img style="vertical-align:middle;" src="images/eqn_0031.svg" alt=""/>, and <img style="vertical-align:middle;" src="images/eqn_0032.svg" alt=""/> is the floor operator, or the largest integer less than or equal to the argument. From this definition, the easiest&mdash;and least efficient&mdash;way to compute the median is to sort the list of sample elements and take the value of the central element(s) to compute the median. Much better methods exist, based on quick selection algorithms <sup><a href="#__reference_8__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 8]<br/>
Thomas H. Cormen et al. (2009), <em>Introduction to Algorithms</em>, 3rd Ed., MIT Press, &sect; 9.3, pp. 220&ndash;222">[8]</a></sup> <sup><a href="#__reference_9__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 9]<br/>
Robert Sedgewick, Kevin Wayne (2011), <em>Algorithms</em>, 4th Ed., Addison-Wesley Professional, pp. 345&ndash;347">[9]</a></sup> and hard-coded selection networks <sup><a href="#__reference_10__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 10]<br/>
Donald E. Knuth (1973), <em>The Art of Computer Programming, Volume 3: Sorting and Searching</em>, Addison Wesley.">[10]</a></sup> <sup><a href="#__reference_11__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 11]<br/>
W. D. Hillis (1992), <em>Co-evolving parasites improve simulated evolution as an optimization procedure</em>, Langton, C. et al. (Eds.), Artificial Life II. Addison Wesley.">[11]</a></sup> <sup><a href="#__reference_12__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 12]<br/>
Hugues Juill√© (1995), <em>Evolution of Non-Deterministic Incremental Algorithms as a New Approach for Search in State Spaces</em>">[12]</a></sup> for small vector lengths, which we use intensively in our implementations.</p>
<p>To know the SNR improvement that can be expected from a median combination of images, we need to know the standard error of the median and compare it to the standard error of the mean. For a large sample of length N and <a href="http://math.stackexchange.com/questions/173337/difference-between-population-sample-and-sample-value" title="http://math.stackexchange.com/questions/173337/difference-between-population-sample-and-sample-value">population</a> median <img style="vertical-align:middle;" src="images/eqn_0024.svg" alt=""/>, the <a href="http://en.wikipedia.org/wiki/Median#Variance" title="http://en.wikipedia.org/wiki/Median#Variance">asymptotic variance formula</a> gives the variance of the sample median <img style="vertical-align:middle;" src="images/eqn_0033.svg" alt=""/>:</p>
<a id="__equation_8__"></a><div class="pidoc_equation"><img src="images/eqn_0034.svg" alt=""/><span class="pidoc_equation_number">[8]</span></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0035.svg" alt=""/> is the distribution's density function. For a <a href="http://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution" title="http://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution">standard normal distribution</a>,</p>
<a id="__equation_9__"></a><div class="pidoc_equation"><img src="images/eqn_0036.svg" alt=""/><span class="pidoc_equation_number">[9]</span></div>
<p>Since the standard deviation of the sample mean is <img style="vertical-align:middle;" src="images/eqn_0037.svg" alt=""/>, the standard deviation of the sample median is larger by a factor of <img style="vertical-align:middle;" src="images/eqn_0038.svg" alt=""/>. From this value we can express the SNR increase for a median combination as</p>
<a id="__equation_10__"></a><div class="pidoc_equation"><img src="images/eqn_0039.svg" alt=""/><span class="pidoc_equation_number">[10]</span></div>
<p>By comparing equations 10 and <a href="#__equation_6__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 6]<br/>
<img src='images/eqn_0022.svg' alt=''/>">[6]</a>, we see that the SNR achieved by a median combination is approximately a 20% less than the SNR of the average combination of the same images (even less for small sets of images <sup><a href="#__reference_16__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 16]<br/>
J. W. McKean, R. M. Schrader (1984), <em>A Comparison of Methods for Studentizing the Sample Median</em>, Communications in Statistics &ndash; Simulation and Computation, 13, pp. 751&ndash;773">[16]</a></sup>). In terms of SNR improvement, average combination is <em>always</em> better, so what can a median combination be useful for? The answer leads to the subject of <a href="http://en.wikipedia.org/wiki/Robust_estimator" title="http://en.wikipedia.org/wiki/Robust_estimator">robust estimation</a>. For a distribution with a strong <a href="http://en.wikipedia.org/wiki/Central_tendency" title="http://en.wikipedia.org/wiki/Central_tendency">central tendency</a>, the median is a robust estimator of the central value. This makes median combination an efficient method for image combination with implicit rejection of <em>outliers,</em> or pixels with too low or too high values due to spurious data. However, in our implementation we provide several pixel rejection algorithms that achieve similar outlier rejection efficiency and can be used with average combination without sacrificing so much signal.</p>
</dd>
</dl>


<div class="pidoc_figure">
<a id="__figure_1__"></a>
<p><span class="pidoc_figure_title">Figure 1 &mdash;</span>  <strong>A Bootstrap Estimation of the Standard Error of the Median</strong></p>
<p>In equations <a href="#__equation_8__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 8]<br/>
<img src='images/eqn_0034.svg' alt=''/>">[8]</a> and <a href="#__equation_9__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 9]<br/>
<img src='images/eqn_0036.svg' alt=''/>">[9]</a> we have derived an analytical expression for the standard error of the median. To improve your understanding of how standard errors work with real sampled data, we'll show you a method to find the standard error of the median experimentally with the help of a script in PixInsight. In this method we generate a large number of observations from the same distribution, compute their medians and means, and compare the standard deviation of the set of medians with respect to the set of means. If the sample size and the number of observations are large enough, this method can be used to derive a good estimate of the standard error of the median&mdash;actually, of the standard error of virtually <em>any</em> statistical estimator with respect to any distribution. This procedure is similar to a <a href="http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29" title="http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29">bootstrap</a> estimation method, <sup><a href="#__reference_13__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 13]<br/>
Rand R. Wilcox (2012), <em>Introduction to Robust Estimation and Hypothesis Testing, 3rd Edition</em>, Elsevier Inc., &sect; 3.1.">[13]</a></sup> but applied to a known distribution. The following script implements this powerful technique.</p>

<pre class="code"><span class="pidoc_sh_multiLineComment">/*
 * Returns a random sample of size n from a standard normal distribution.
 *
 * Generates n random normal deviates using the Marsaglia polar method:
 *    http://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform
 *    http://en.wikipedia.org/wiki/Marsaglia_polar_method
 */</span>
<span class="pidoc_sh_keyword">function</span> <span class="pidoc_sh_function">normalSample</span>( n )
{
   <span class="pidoc_sh_keyword">var</span> g = <span class="pidoc_sh_keyword">new</span> <span class="pidoc_sh_object">Vector</span>( n );
   <span class="pidoc_sh_keyword">for</span> ( <span class="pidoc_sh_keyword">var</span> i = <span class="pidoc_sh_number">0</span>; i &lt; n; ++i )
   {
      <span class="pidoc_sh_keyword">var</span> s, u, v; <span class="pidoc_sh_singleLineComment">// I love SUVs :)</span>
      <span class="pidoc_sh_keyword">do</span>
      {
         u = <span class="pidoc_sh_number">2</span>*<span class="pidoc_sh_object">Math</span>.<span class="pidoc_sh_function">random</span>() - <span class="pidoc_sh_number">1</span>;
         v = <span class="pidoc_sh_number">2</span>*<span class="pidoc_sh_object">Math</span>.<span class="pidoc_sh_function">random</span>() - <span class="pidoc_sh_number">1</span>;
         s = u*u + v*v;
      }
      <span class="pidoc_sh_keyword">while</span> ( s &gt;= <span class="pidoc_sh_number">1</span> || s == <span class="pidoc_sh_number">0</span> );
      s = <span class="pidoc_sh_object">Math</span>.<span class="pidoc_sh_function">sqrt</span>( -<span class="pidoc_sh_number">2</span>*<span class="pidoc_sh_object">Math</span>.<span class="pidoc_sh_function">ln</span>( s )/s );
      g.<span class="pidoc_sh_function">at</span>( i, u*s );
      <span class="pidoc_sh_keyword">if</span> ( ++i &lt; n )
         g.<span class="pidoc_sh_function">at</span>( i, v*s );
   }
   <span class="pidoc_sh_keyword">return</span> g;
}

<span class="pidoc_sh_multiLineComment">/*
 * Bootstrap estimation
 */</span>

<span class="pidoc_sh_preprocessor">#define sampleSize      2000</span>
<span class="pidoc_sh_preprocessor">#define numberOfSamples 2000</span>
<span class="pidoc_sh_preprocessor">#define numberOfTests   1000</span>

console.<span class="pidoc_sh_function">show</span>();
console.<span class="pidoc_sh_function">writeln</span>( <span class="pidoc_sh_quotation">"&lt;end&gt;&lt;cbr&gt;Performing bootstrap, please wait..."</span> );

<span class="pidoc_sh_keyword">var</span> sigma_a = <span class="pidoc_sh_keyword">new</span> <span class="pidoc_sh_object">Vector</span>( numberOfTests );
<span class="pidoc_sh_keyword">var</span> sigma_m = <span class="pidoc_sh_keyword">new</span> <span class="pidoc_sh_object">Vector</span>( numberOfTests );
<span class="pidoc_sh_keyword">for</span> ( <span class="pidoc_sh_keyword">var</span> j = <span class="pidoc_sh_number">0</span>; j &lt; numberOfTests; ++j )
{
   <span class="pidoc_sh_keyword">var</span> a = <span class="pidoc_sh_keyword">new</span> <span class="pidoc_sh_object">Vector</span>( numberOfSamples );
   <span class="pidoc_sh_keyword">var</span> m = <span class="pidoc_sh_keyword">new</span> <span class="pidoc_sh_object">Vector</span>( numberOfSamples );
   <span class="pidoc_sh_keyword">for</span> ( <span class="pidoc_sh_keyword">var</span> i = <span class="pidoc_sh_number">0</span>; i &lt; numberOfSamples; ++i )
   {
     <span class="pidoc_sh_keyword">var</span> x = <span class="pidoc_sh_function">normalSample</span>( sampleSize );
     a.<span class="pidoc_sh_function">at</span>( i, x.<span class="pidoc_sh_function">mean</span>() );
     m.<span class="pidoc_sh_function">at</span>( i, x.<span class="pidoc_sh_function">median</span>() );
   }
   sigma_a.<span class="pidoc_sh_function">at</span>( j, a.<span class="pidoc_sh_function">stdDev</span>() );
   sigma_m.<span class="pidoc_sh_function">at</span>( j, m.<span class="pidoc_sh_function">stdDev</span>() );
}

<span class="pidoc_sh_keyword">var</span> aa = sigma_a.<span class="pidoc_sh_function">mean</span>();
<span class="pidoc_sh_keyword">var</span> ma = sigma_m.<span class="pidoc_sh_function">mean</span>();
console.<span class="pidoc_sh_function">writeln</span>( <span class="pidoc_sh_function">format</span>( <span class="pidoc_sh_quotation">"&lt;end&gt;&lt;cbr&gt;Stddev of the mean ......... %.6f"</span>, aa ) );
console.<span class="pidoc_sh_function">writeln</span>( <span class="pidoc_sh_function">format</span>(           <span class="pidoc_sh_quotation">"Stddev of the median ....... %.6f"</span>, ma ) );
console.<span class="pidoc_sh_function">writeln</span>( <span class="pidoc_sh_function">format</span>(           <span class="pidoc_sh_quotation">"Median/mean stddev ratio ... %.3f"</span>, ma/aa ) );
<span class="pidoc_sh_multiLineComment">/*
Result:
Stddev of the mean ......... 0.022350
Stddev of the median ....... 0.028001
Median/mean stddev ratio ... 1.253
*/</span></pre>

<p>The script generates sets of 2000 samples of 2000 random deviates from a standard normal distribution, stores their median and mean values as two vectors, and computes their standard deviations. After repeating this experiment 1000 times, the script gives an estimate of the ratio between the standard deviations of the median and the mean for a normal distribution. The average ratio tends to stabilize around 1.253 approximately. As expected, this is the same value we found analytically in Equation <a href="#__equation_9__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 9]<br/>
<img src='images/eqn_0036.svg' alt=''/>">[9]</a>.</p>
<p>You can use this script as a starting point to perform interesting analyses. For example, you can reduce the value of the <span class="pidoc_code">sampleSize</span> macro to study the variation of the standard error of the median as a function of the sample size. With a little more work, you can transform the script to evaluate the error and efficiency of other estimators for different distributions.</p>
</div>

<div class="pidoc_vspacer" style="margin-top:2em;"></div>

<div class="pidoc_figure">
<a id="__figure_2__"></a>
<p><span class="pidoc_figure_title">Figure 2 &mdash;</span>  <strong>An Image Integration Experiment</strong></p>
<div class="pidoc_mouseover">
<div class="pidoc_image_right"><img src="images/integration_original.png" id="4GDTU8H8JG3m2ltv" alt="" /></div>
<ul>
<li><span class="pidoc_indicator_default" id="4GDTU8H8JG3m2ltv_1"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('4GDTU8H8JG3m2ltv', 'images/integration_original.png'); pidoc_hideGroup('4GDTU8H8JG3m2ltv', 13); pidoc_setOpacity('4GDTU8H8JG3m2ltv_1', 1.0);">Original image</a></li>
<li><span class="pidoc_indicator" id="4GDTU8H8JG3m2ltv_2"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('4GDTU8H8JG3m2ltv', 'images/integration_noise.png'); pidoc_hideGroup('4GDTU8H8JG3m2ltv', 13); pidoc_setOpacity('4GDTU8H8JG3m2ltv_2', 1.0);">A sample of uniform random noise</a></li>
<li><span class="pidoc_indicator" id="4GDTU8H8JG3m2ltv_3"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('4GDTU8H8JG3m2ltv', 'images/integration_of_1.png'); pidoc_hideGroup('4GDTU8H8JG3m2ltv', 13); pidoc_setOpacity('4GDTU8H8JG3m2ltv_3', 1.0);">Original image + noise</a></li>
<li><span class="pidoc_indicator" id="4GDTU8H8JG3m2ltv_4"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('4GDTU8H8JG3m2ltv', 'images/integration_of_2.png'); pidoc_hideGroup('4GDTU8H8JG3m2ltv', 13); pidoc_setOpacity('4GDTU8H8JG3m2ltv_4', 1.0);">Integration of 2 noisy images</a></li>
<li><span class="pidoc_indicator" id="4GDTU8H8JG3m2ltv_5"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('4GDTU8H8JG3m2ltv', 'images/integration_of_4.png'); pidoc_hideGroup('4GDTU8H8JG3m2ltv', 13); pidoc_setOpacity('4GDTU8H8JG3m2ltv_5', 1.0);">Integration of 4 noisy images</a></li>
<li><span class="pidoc_indicator" id="4GDTU8H8JG3m2ltv_6"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('4GDTU8H8JG3m2ltv', 'images/integration_of_8.png'); pidoc_hideGroup('4GDTU8H8JG3m2ltv', 13); pidoc_setOpacity('4GDTU8H8JG3m2ltv_6', 1.0);">Integration of 8 noisy images</a></li>
<li><span class="pidoc_indicator" id="4GDTU8H8JG3m2ltv_7"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('4GDTU8H8JG3m2ltv', 'images/integration_of_16.png'); pidoc_hideGroup('4GDTU8H8JG3m2ltv', 13); pidoc_setOpacity('4GDTU8H8JG3m2ltv_7', 1.0);">Integration of 16 noisy images</a></li>
<li><span class="pidoc_indicator" id="4GDTU8H8JG3m2ltv_8"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('4GDTU8H8JG3m2ltv', 'images/integration_of_32.png'); pidoc_hideGroup('4GDTU8H8JG3m2ltv', 13); pidoc_setOpacity('4GDTU8H8JG3m2ltv_8', 1.0);">Integration of 32 noisy images</a></li>
<li><span class="pidoc_indicator" id="4GDTU8H8JG3m2ltv_9"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('4GDTU8H8JG3m2ltv', 'images/integration_of_64.png'); pidoc_hideGroup('4GDTU8H8JG3m2ltv', 13); pidoc_setOpacity('4GDTU8H8JG3m2ltv_9', 1.0);">Integration of 64 noisy images</a></li>
<li><span class="pidoc_indicator" id="4GDTU8H8JG3m2ltv_10"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('4GDTU8H8JG3m2ltv', 'images/integration_of_128.png'); pidoc_hideGroup('4GDTU8H8JG3m2ltv', 13); pidoc_setOpacity('4GDTU8H8JG3m2ltv_10', 1.0);">Integration of 128 noisy images</a></li>
<li><span class="pidoc_indicator" id="4GDTU8H8JG3m2ltv_11"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('4GDTU8H8JG3m2ltv', 'images/integration_of_256.png'); pidoc_hideGroup('4GDTU8H8JG3m2ltv', 13); pidoc_setOpacity('4GDTU8H8JG3m2ltv_11', 1.0);">Integration of 256 noisy images</a></li>
<li><span class="pidoc_indicator" id="4GDTU8H8JG3m2ltv_12"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('4GDTU8H8JG3m2ltv', 'images/integration_of_512.png'); pidoc_hideGroup('4GDTU8H8JG3m2ltv', 13); pidoc_setOpacity('4GDTU8H8JG3m2ltv_12', 1.0);">Integration of 512 noisy images</a></li>
<li><span class="pidoc_indicator" id="4GDTU8H8JG3m2ltv_13"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('4GDTU8H8JG3m2ltv', 'images/integration_of_1024.png'); pidoc_hideGroup('4GDTU8H8JG3m2ltv', 13); pidoc_setOpacity('4GDTU8H8JG3m2ltv_13', 1.0);">Integration of 1024 noisy images</a></li>
</ul>
</div>

<div class="pidoc_vspacer" style="margin-top:0.5em;"></div>
<p><a href="http://en.wikipedia.org/wiki/Uniform_distribution_%28continuous%29" title="http://en.wikipedia.org/wiki/Uniform_distribution_%28continuous%29">Uniformly distributed</a> random noise was added to 1024 duplicates of the original image, and 10 subsets of the noisy images were averaged following a dyadic sequence. The result can be seen on the comparison above. To carry out this experiment we used the following simple script in PixInsight. To run the script, you have to set the values of the BITMAP_FILE_PATH and OUTPUT_DIR macros to the file path of your original image (the TIFF, PNG, BMP, JPEG, GIF, PPM and XPM formats are supported) and the directory where output files (in PNG format) will be written, respectively.<br class="pidoc_clearfix"/></p>

<div class="pidoc_vspacer" style="margin-top:0.5em;"></div>

<pre class="code"><span class="pidoc_sh_multiLineComment">/*
 * An Image Integration Experiment
 */</span>

<span class="pidoc_sh_preprocessor">#include &lt;pjsr/ImageOp.jsh&gt;</span>
<span class="pidoc_sh_preprocessor">#include &lt;pjsr/UndoFlag.jsh&gt;</span>

<span class="pidoc_sh_singleLineComment">// </span><span class="pidoc_sh_alert">N.B.</span><span class="pidoc_sh_singleLineComment"> Change to the actual file path of the test bitmap.</span>
<span class="pidoc_sh_preprocessor">#define BITMAP_FILE_PATH   "/path/to/original/image.png"</span>

<span class="pidoc_sh_singleLineComment">// </span><span class="pidoc_sh_alert">N.B.</span><span class="pidoc_sh_singleLineComment"> Change to the actual directory where output files will be written.</span>
<span class="pidoc_sh_preprocessor">#define OUTPUT_DIR         "/path/to/output/directory"</span>

<span class="pidoc_sh_preprocessor">#define NUMBER_OF_IMAGES   1024</span>

<span class="pidoc_sh_keyword">var</span> bmp = <span class="pidoc_sh_keyword">new</span> <span class="pidoc_sh_object">Bitmap</span>( BITMAP_FILE_PATH );

<span class="pidoc_sh_keyword">var</span> noiseGenerator = <span class="pidoc_sh_keyword">new</span> <span class="pidoc_sh_externalObject">NoiseGenerator</span>;
noiseGenerator.amount = <span class="pidoc_sh_number">1.00</span>;
<span class="pidoc_sh_singleLineComment">//noiseGenerator.distribution = NoiseGenerator.prototype.Normal;</span>
noiseGenerator.distribution = <span class="pidoc_sh_externalObject">NoiseGenerator</span>.<span class="pidoc_sh_keyword">prototype</span>.Uniform;
noiseGenerator.preserveBrightness = <span class="pidoc_sh_externalObject">NoiseGenerator</span>.<span class="pidoc_sh_keyword">prototype</span>.None;

<span class="pidoc_sh_keyword">var</span> noise = <span class="pidoc_sh_keyword">new</span> <span class="pidoc_sh_object">ImageWindow</span>( bmp.width, bmp.height, <span class="pidoc_sh_number">1</span>, <span class="pidoc_sh_number">32</span>, <span class="pidoc_sh_keyword">true</span>, <span class="pidoc_sh_keyword">false</span> );

<span class="pidoc_sh_keyword">var</span> sum = <span class="pidoc_sh_keyword">new</span> <span class="pidoc_sh_object">Image</span>( bmp.width, bmp.height );
sum.<span class="pidoc_sh_function">fill</span>( <span class="pidoc_sh_number">0</span> );

<span class="pidoc_sh_keyword">for</span> ( <span class="pidoc_sh_keyword">var</span> n0 = <span class="pidoc_sh_number">0</span>, n = <span class="pidoc_sh_number">1</span>; n &lt;= NUMBER_OF_IMAGES; n0 = n, n *= <span class="pidoc_sh_number">2</span> )
{
   <span class="pidoc_sh_keyword">for</span> ( <span class="pidoc_sh_keyword">var</span> i = n0; i &lt; n; ++i )
   {
      noiseGenerator.<span class="pidoc_sh_function">executeOn</span>( noise.mainView, <span class="pidoc_sh_keyword">false</span><span class="pidoc_sh_multiLineComment">/*swapFile*/</span> );
      <span class="pidoc_sh_keyword">var</span> noisyImage = <span class="pidoc_sh_keyword">new</span> <span class="pidoc_sh_object">Image</span>( bmp.width, bmp.height );
      noisyImage.<span class="pidoc_sh_function">blend</span>( bmp );
      noisyImage.<span class="pidoc_sh_function">apply</span>( noise.mainView.image, ImageOp_Add );
      sum.<span class="pidoc_sh_function">apply</span>( noisyImage, ImageOp_Add );
      noisyImage.<span class="pidoc_sh_function">free</span>();
   }

   <span class="pidoc_sh_keyword">var</span> id = <span class="pidoc_sh_function">format</span>( <span class="pidoc_sh_quotation">"integration_of_%d"</span>, n );
   <span class="pidoc_sh_keyword">var</span> window = <span class="pidoc_sh_keyword">new</span> <span class="pidoc_sh_object">ImageWindow</span>( sum.width, sum.height, <span class="pidoc_sh_number">1</span>, <span class="pidoc_sh_number">32</span>, <span class="pidoc_sh_keyword">true</span>, <span class="pidoc_sh_keyword">false</span>, id );
   <span class="pidoc_sh_keyword">var</span> view = window.mainView;
   view.<span class="pidoc_sh_function">beginProcess</span>( UndoFlag_NoSwapFile );
   view.image.<span class="pidoc_sh_function">apply</span>( sum, ImageOp_Mov );
   view.image.<span class="pidoc_sh_function">rescale</span>();
   view.<span class="pidoc_sh_function">endProcess</span>();
   window.<span class="pidoc_sh_function">show</span>();
   window.<span class="pidoc_sh_function">zoomToFit</span>();
   view.image.<span class="pidoc_sh_function">render</span>().<span class="pidoc_sh_function">save</span>( OUTPUT_DIR + <span class="pidoc_sh_quotation">"/"</span> + id + <span class="pidoc_sh_quotation">".png"</span> );
}

noise.<span class="pidoc_sh_function">forceClose</span>();</pre>

</div>

<div class="pidoc_vspacer" style="margin-top:2em;"></div>

<div class="pidoc_figure">
<a id="__figure_3__"></a>
<p><span class="pidoc_figure_title">Figure 3 &mdash;</span>  <strong>How many images?</strong></p>
<img src="images/DeltaSNRGraph.svg" alt=""/>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>
<p>This graph plots the function</p>
<div class="pidoc_equation"><img src="images/eqn_0040.svg" alt=""/></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0041.svg" alt=""/> is the number of integrated images. This function represents the SNR improvement achieved by each new image added to an integration stack. The function decreases as more images are added, approaching zero asymptotically. The improvement remains significant up to about 30 images. Above 30 images, a considerable imaging effort is required to achieve a noticeable SNR increment. Beyond 50 frames, the task becomes impractical for long-exposure deep-sky images.</p>
</div>
</div>

<div class="pidoc_subsection" id="__Description_:_Image_Weighting__">
   <h4 class="pidoc_subsectionTitle">1.2&emsp;Image Weighting</h4>
<p>In the preceding section we have described average image combination on a simplified basis, assuming that all of the images contribute equally to the output integrated image. Actually this does not lead, in general, to an optimal combination in terms of SNR improvement. For example, suppose that one of the images being combined has more noise than the rest, or in other terms, its SNR is relatively low. If we simply combine the noisier image, it will degrade the result because the precondition assumed in Equation <a href="#__equation_2__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 2]<br/>
<img src='images/eqn_0005.svg' alt=''/>">[2]</a>&mdash;that the signal has the same relative strength in all of the images&mdash;won't be true. As a result of this precondition violation, part of the noise in the faulty image will be treated as if it were signal, degrading the result.</p>
<p>To maximize SNR in the integrated image we assign a multiplicative weighting factor to each input image. Image weights should account as accurately as possible for the existing SNR differences in the original data set. The weighted average combination is therefore</p>
<a id="__equation_11__"></a><div class="pidoc_equation"><img src="images/eqn_0042.svg" alt=""/><span class="pidoc_equation_number">[11]</span></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0043.svg" alt=""/> is the weight assigned to the image <img style="vertical-align:middle;" src="images/eqn_0006.svg" alt=""/>. We have implemented several methods to define image weights in a flexible fashion adapted to different imaging scenarios. The default image weighting method implemented in the ImageIntegration tool is <em>multiscale noise evaluation</em>, which we describe below.</p>

<dl class="pidoc_list">
<dt>
<p><a id="noise_evaluation_weighting"></a> Noise Evaluation Weighting</p>
</dt>
<dd>
<p>In this method we compute robust noise estimates for each input image, and use them to minimize mean square error in the integrated result. This method has proven accurate and efficient, and works in a completely automatic way exclusively from existing image data, without requiring additional information such as exposure times or sensor parameters.</p>
<p>The implemented noise estimation algorithm is based on a multiscale data structure known as <em>multiresolution support</em> (MRS), and has been described in References <sup><a href="#__reference_1__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 1]<br/>
Jean-Luc Starck and Fionn Murtagh (1998), <em>Automatic Noise Estimation from the Multiresolution Support</em>, Publications of the Royal Astronomical Society of the Pacific, vol. 110, pp. 193&ndash;199">[1]</a></sup> and <sup><a href="#__reference_2__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 2]<br/>
Jean-Luc Starck and Fionn Murtagh (2002), <em>Astronomical Image and Data Analysis</em>, Springer, pp. 36&ndash;39">[2]</a></sup>. The original algorithm can fail to converge for images with very low noise levels, or images degraded at the one-pixel scale such as RAW DSLR frames demosaiced with bilinear interpolation. For improved robustness and accuracy, we iterate the MRS noise evaluation algorithm to find the highest wavelet layer where the algorithm converges for scales from 1 to 8 pixels. Finally, in the rare cases where MRS noise evaluation does not converge, we fall back to a k-sigma noise estimation scheme, which &quot;cannot fail&quot; to compute a less accurate but still robust and useful noise estimate. The following script summarizes these concepts.</p>

<pre class="code"><span class="pidoc_sh_multiLineComment">/*
 * Estimation of the standard deviation of the noise, assuming a Gaussian
 * noise distribution.
 *
 * - Use MRS noise evaluation when the algorithm converges for 4 &gt;= J &gt;= 2
 *
 * - Use k-sigma noise evaluation when either MRS doesn't converge or the
 *   length of the noise pixels set is below a 1% of the image area.
 *
 * - Automatically iterate to find the highest layer where noise can be
 *   successfully evaluated, in the [1,3] range.
 */</span>
<span class="pidoc_sh_keyword">function</span> <span class="pidoc_sh_function">NoiseEvaluation</span>( img )
{
   <span class="pidoc_sh_keyword">var</span> a, n = <span class="pidoc_sh_number">4</span>, m = <span class="pidoc_sh_number">0.01</span>*img.selectedRect.area;
   <span class="pidoc_sh_keyword">for</span> ( ;; )
   {
      a = img.<span class="pidoc_sh_function">noiseMRS</span>( n );
      <span class="pidoc_sh_keyword">if</span> ( a[<span class="pidoc_sh_number">1</span>] &gt;= m )
         <span class="pidoc_sh_keyword">break</span>;
      <span class="pidoc_sh_keyword">if</span> ( --n == <span class="pidoc_sh_number">1</span> )
      {
         console.<span class="pidoc_sh_function">writeln</span>( <span class="pidoc_sh_quotation">"&lt;end&gt;&lt;cbr&gt;** Warning: No convergence in MRS noise evaluation routine"</span> +
                          <span class="pidoc_sh_quotation">" - using k-sigma noise estimate."</span> );
         a = img.<span class="pidoc_sh_function">noiseKSigma</span>();
         <span class="pidoc_sh_keyword">break</span>;
      }
   }
   <span class="pidoc_sh_keyword">this</span>.sigma = a[<span class="pidoc_sh_number">0</span>]; <span class="pidoc_sh_singleLineComment">// estimated stddev of Gaussian noise</span>
   <span class="pidoc_sh_keyword">this</span>.count = a[<span class="pidoc_sh_number">1</span>]; <span class="pidoc_sh_singleLineComment">// number of pixels in the noise pixels set</span>
   <span class="pidoc_sh_keyword">this</span>.layers = n;   <span class="pidoc_sh_singleLineComment">// number of layers used for noise evaluation</span>
}

<span class="pidoc_sh_keyword">function</span> <span class="pidoc_sh_function">main</span>()
{
   <span class="pidoc_sh_singleLineComment">// Get access to the current active image window.</span>
   <span class="pidoc_sh_keyword">var</span> window = <span class="pidoc_sh_object">ImageWindow</span>.activeWindow;
   <span class="pidoc_sh_keyword">if</span> ( window.isNull )
      <span class="pidoc_sh_keyword">throw</span> <span class="pidoc_sh_keyword">new</span> <span class="pidoc_sh_object">Error</span>( <span class="pidoc_sh_quotation">"No active image"</span> );

   console.<span class="pidoc_sh_function">show</span>();
   console.<span class="pidoc_sh_function">writeln</span>( <span class="pidoc_sh_quotation">"&lt;end&gt;&lt;cbr&gt;&lt;br&gt;&lt;b&gt;"</span> + window.currentView.fullId + <span class="pidoc_sh_quotation">"&lt;/b&gt;"</span> );
   console.<span class="pidoc_sh_function">writeln</span>( <span class="pidoc_sh_quotation">"Calculating noise standard deviation..."</span> );
   console.<span class="pidoc_sh_function">flush</span>();

   console.abortEnabled = <span class="pidoc_sh_keyword">true</span>;

   <span class="pidoc_sh_singleLineComment">// Compute noise estimates for the active view.</span>
   <span class="pidoc_sh_keyword">var</span> img = window.currentView.image;
   <span class="pidoc_sh_keyword">for</span> ( <span class="pidoc_sh_keyword">var</span> c = <span class="pidoc_sh_number">0</span>; c &lt; img.numberOfChannels; ++c )
   {
      console.<span class="pidoc_sh_function">writeln</span>( <span class="pidoc_sh_quotation">"&lt;end&gt;&lt;cbr&gt;&lt;br&gt;* Channel #"</span>, c );
      console.<span class="pidoc_sh_function">flush</span>();
      img.selectedChannel = c;
      <span class="pidoc_sh_keyword">var</span> E = <span class="pidoc_sh_keyword">new</span> <span class="pidoc_sh_function">NoiseEvaluation</span>( img );
      console.<span class="pidoc_sh_function">writeln</span>( <span class="pidoc_sh_function">format</span>( <span class="pidoc_sh_quotation">"sigma%c = %.3e, N = %u (%.2f%%), J = %d"</span>,
                               img.isColor ? <span class="pidoc_sh_quotation">"RGB"</span>[c] : <span class="pidoc_sh_quotation">'K'</span>,
                               E.sigma, E.count, <span class="pidoc_sh_number">100</span>*E.count/img.selectedRect.area, E.layers ) );
      console.<span class="pidoc_sh_function">flush</span>();
   }
}

<span class="pidoc_sh_function">main</span>();</pre>


<div class="pidoc_vspacer" style="margin-top:2em;"></div>
<p>For a given function <img style="vertical-align:middle;" src="images/eqn_0044.svg" alt=""/> and a set of measured function values <img style="vertical-align:middle;" src="images/eqn_0045.svg" alt=""/>, define <em>mean square error</em> as</p>
<div class="pidoc_equation"><img src="images/eqn_0046.svg" alt=""/></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0047.svg" alt=""/> is the expected value of the argument. We define the signal-to-noise ratio function (SNR) as the ratio of the mean square sample value to the mean square error:</p>
<a id="__equation_12__"></a><div class="pidoc_equation"><img src="images/eqn_0048.svg" alt=""/><span class="pidoc_equation_number">[12]</span></div>
<p>Our goal in the noise evaluation weighting method is to minimize mean square error in the integrated image or, equivalently, maximize its signal-to-noise ratio. SNR, as defined by the above equation, suggests itself as an image weighting function: the mean square error can be approximated by the variance of the noise computed with the MRS algorithm, and calculating the mean square sample value is relatively trivial. Unfortunately, the SNR function has two main problems:</p>

<ul class="pidoc_list">
<li>As a signal estimator, the numerator of the SNR equation (mean square sample) is not robust. In fact, its <a href="http://en.wikipedia.org/wiki/Breakdown_point#Breakdown_point" title="http://en.wikipedia.org/wiki/Breakdown_point#Breakdown_point">breakdown point</a> is zero, which makes it extremely unstable. Attempts to use trimmed and Winsorized versions of the mean square fail in this case because the estimator becomes insufficient, and <a href="http://en.wikipedia.org/wiki/Sufficient_statistic" title="http://en.wikipedia.org/wiki/Sufficient_statistic">sufficiency</a> is a crucial property here. To properly implement SNR as a weighting function, image weights should be computed after pixel rejection, which involves <img style="vertical-align:middle;" src="images/eqn_0049.svg" alt=""/> additional storage space requirements, <img style="vertical-align:middle;" src="images/eqn_0002.svg" alt=""/> being the number of integrated images and <img style="vertical-align:middle;" src="images/eqn_0050.svg" alt=""/> the number of pixels. Even after rejection and mormalization, the mean square sample is too sensitive to outliers, which degrades its accuracy as a signal estimator.</li>
<li class="pidoc_spaced_list_item">As a result of its lack of robustness, the SNR function is even more inaccurate in presence of sky gradients, which further limits its applicability.</li>
</ul>

<p>To overcome these problems we use a different, closely related but robust image weighting functional:</p>
<a id="__equation_13__"></a><div class="pidoc_equation"><img src="images/eqn_0051.svg" alt=""/><span class="pidoc_equation_number">[13]</span></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0052.svg" alt=""/> is the standard deviation of the noise in the <img style="vertical-align:middle;" src="images/eqn_0053.svg" alt=""/> input image <img style="vertical-align:middle;" src="images/eqn_0006.svg" alt=""/>, given by the MRS algorithm, and <img style="vertical-align:middle;" src="images/eqn_0054.svg" alt=""/> is the scaling factor for <img style="vertical-align:middle;" src="images/eqn_0006.svg" alt=""/>, given by Equation <a href="#__equation_34__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 34]<br/>
<img src='images/eqn_0131.svg' alt=''/>">[34]</a>.</p>
<p>The above weighting function is robust and efficient, and works well even when the images include relatively strong gradients. Equation 13 defines the weighting factor <img style="vertical-align:middle;" src="images/eqn_0043.svg" alt=""/> that enters Equation <a href="#__equation_11__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 11]<br/>
<img src='images/eqn_0042.svg' alt=''/>">[11]</a> for weighted average combination. Our tests have shown that the efficiency of this weighting scheme is similar to the SNR function in terms of SNR maximization (inferior by only a 1% - 3%, depending on the quality of the data). The reliability and robustness of our weighting function more than compensate for a slightly worse performance.</p>
</dd>
</dl>

</div>

<div class="pidoc_subsection" id="__Description_:_Pixel_Rejection__">
   <h4 class="pidoc_subsectionTitle">1.3&emsp;Pixel Rejection</h4>
<p>At the beginning of this document, we have said that the set of integrated images usually contains spurious data that cannot be characterized as random noise. Here we are interested in accidental phenomena such as plane or satellite trails and cosmic ray impacts on CCD and CMOS sensors, and also in instrumental defects such as hot or cold pixels and bad pixel rows and columns (with the necessary help of some dithering between subexposures!). All of these <em>bad data</em> form bright or dark artifacts at relatively small dimensional scales, which can be removed efficiently during the image integration task thanks to a family of statistical methods collectively known as <em>pixel rejection algorithms</em>.</p>
<p>The goal of a pixel rejection algorithm is to exclude <a href="http://en.wikipedia.org/wiki/Outlier" title="http://en.wikipedia.org/wiki/Outlier">outliers</a> from the set of pixels that are to be combined in each pixel stack. The differences between the rejection algorithms available lay basically in their sophistication and suitability to detect true outliers in small and large sets of images. In the following paragraphs we describe the rejection algorithms currently implemented in our ImageIntegration tool.</p>
<p><a id="minmax_clipping"></a></p>

<dl class="pidoc_list">
<dt>
<p>Min/Max Clipping</p>
</dt>
<dd>
<p>The simplest rejection algorithm is known as <em>min/max.</em> It simply excludes the <img style="vertical-align:middle;" src="images/eqn_0055.svg" alt=""/> smallest and the <img style="vertical-align:middle;" src="images/eqn_0056.svg" alt=""/> largest pixels from every pixel stack. While this is an efficient method for rejection of outliers&mdash;indeed it <em>cannot fail</em> to reject them&mdash;, it is quite poor in terms of signal preservation. For an <a href="#average_combination">average combination</a>, min/max rejection leads to a <em>constant</em> SNR loss proportional to the square root of the number of clipped pixels. For example, if you set <img style="vertical-align:middle;" src="images/eqn_0057.svg" alt=""/>, then the result of min/max in SNR terms is the same as if you removed two images from your data set and used a better rejection algorithm such as sigma clipping.</p>
<p>Min/max clipping can be used as a counter-test to verify the efficiency of fine tuned rejection parameters for more efficient rejection algorithms. Other than these control tasks and some special cases, min/max should be avoided in production work.</p>
</dd>
<dt>
<p><a id="percentile_clipping"></a> Percentile Clipping</p>
</dt>
<dd>
<p>Percentile clipping is a one-step rejection algorithm where a pixel is excluded from a pixel stack if it falls outside a range defined in terms of the stack's <a href="http://en.wikipedia.org/wiki/Central_tendency" title="http://en.wikipedia.org/wiki/Central_tendency">central value</a>. The algorithm can be formalized as</p>
<a id="__equation_14__"></a><div class="pidoc_equation"><img src="images/eqn_0058.svg" alt=""/><span class="pidoc_equation_number">[14]</span></div>
<p>and the rejection function</p>
<a id="__equation_15__"></a><div class="pidoc_equation"><img src="images/eqn_0059.svg" alt=""/><span class="pidoc_equation_number">[15]</span></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0060.svg" alt=""/> is the pixel value, <img style="vertical-align:middle;" src="images/eqn_0024.svg" alt=""/> is the median of the pixel stack, and <img style="vertical-align:middle;" src="images/eqn_0061.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0062.svg" alt=""/> are the low and high clipping point parameters, respectively, in the [0,1] range. Percentile clipping is a good choice only for small sets, say up to 6 images maximum. With just a few images, more sophisticated algorithms tend to fail because they are based on statistical moments higher than the mean (e.g., the standard deviation), which have little significance for very small samples.</p>
</dd>
<dt>
<p><a id="sigma_clipping"></a> Sigma Clipping</p>
</dt>
<dd>
<p>Sigma clipping is an iterative rejection algorithm. At each iteration, the central value and dispersion (or <em>sigma</em>) of the pixel stack are estimated, and all pixels whose distances from the central value are larger than a prescribed value in sigma units are rejected:</p>
<a id="__equation_16__"></a><div class="pidoc_equation"><img src="images/eqn_0063.svg" alt=""/><span class="pidoc_equation_number">[16]</span></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0064.svg" alt=""/> is defined as</p>
<a id="__equation_17__"></a><div class="pidoc_equation"><img src="images/eqn_0065.svg" alt=""/><span class="pidoc_equation_number">[17]</span></div>
<p>and <img style="vertical-align:middle;" src="images/eqn_0066.svg" alt=""/> <img style="vertical-align:middle;" src="images/eqn_0024.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0018.svg" alt=""/> are the pixel value, the median and the standard deviation of the current stack, respectively. <img style="vertical-align:middle;" src="images/eqn_0067.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0068.svg" alt=""/> are the clipping point parameters expressed in sigma units. The sigma clipping algorithm requires a minimum of 8 or 10 images. The more images the better, but for larger sets Winsorized sigma clipping and linear fit clipping tend to be more efficient.</p>

<div class="pidoc_figure">
<a id="__figure_4__"></a>
<p><span class="pidoc_figure_title">Figure 4 &mdash;</span>  <strong>Sigma Clipping Rejection</strong></p>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>
<img src="images/SigmaClippingRejectionGraph.svg" alt=""/>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>
<p>The elements of a pixel stack have been sorted in ascending order and represented as circles on this graph. The red line labeled as <em>m</em> represents the median of the pixel stack. The two red lines above and below the median are the clipping points, <img style="vertical-align:middle;" src="images/eqn_0068.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0067.svg" alt=""/> respectively in sigma units. All pixels falling outside the interval between the clipping points&mdash;represented as empty circles in the figure&mdash;will be rejected. However, are all of these rejected pixels true outliers? Compare with <a href="#linear_fit_clipping">linear fit clipping</a> rejection in Figure <a href="#__figure_5__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Figure 5]">[5]</a>.</p>
</div>
</dd>
<dt>
<p><a id="winsorized_sigma_clipping"></a> Winsorized Sigma Clipping</p>
</dt>
<dd>
<p>Winsorization,<sup><a href="#__reference_3__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 3]<br/>
John W. Tukey (1962), <em>The Future of Data Analysis</em>, The Annals of Mathematical Statistics, Vol. 33, No. 1, pp. 17&ndash;19">[3]</a></sup> named after statistician Charles P. Winsor, is a process for robust estimation of sample parameters in presence of outliers. When a sample value falls outside the acceptable range, Winsorization does not simply reject it, but replaces it by the nearest valid neighbor in the sample. Winsorized estimates are usually more robust than estimates from simpler procedures such as trimming or truncation. To understand the Winsorization process, consider the following sorted set of 15 values:</p>
<p><span class="pidoc_code"><strong>9,</strong> 13, 15, 15, 17, 18, 18, 19, 21, 23, 24, 24, 25, <strong>31,</strong> <strong>33</strong></span> ,</p>
<p>where we can identify three outliers: 9 is suspiciously low, and 31 and 33 are clearly too high. Based on this intuition-driven rejection criterion, the Winsorized sample would be:</p>
<p><span class="pidoc_code"><strong>13,</strong> 13, 15, 15, 17, 18, 18, 19, 21, 23, 24, 24, 25, <strong>25,</strong> <strong>25</strong></span> ,</p>
<p>where the outliers have been replaced by their nearest valid neighbor values.</p>
<p>As implemented in our ImageIntegration tool, the Winsorized sigma clipping algorithm applies Winsorization to compute robust estimates of the central value and variability of each pixel stack. These estimates are called <em>Winsorized mean</em> and <em>Winsorized sigma</em>, respectively. The basic process has been described by Huber.<sup><a href="#__reference_4__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 4]<br/>
Peter J. Huber and E. Ronchetti (2009), <em>Robust Statistics</em>, 2nd Ed., Wiley">[4]</a></sup> The implemented algorithm can be formalized as</p>
<a id="__equation_18__"></a><div class="pidoc_equation"><img src="images/eqn_0069.svg" alt=""/><span class="pidoc_equation_number">[18]</span></div>
<p>where the constant <img style="vertical-align:middle;" src="images/eqn_0070.svg" alt=""/> sets the iteration limit for Huber's loop (<img style="vertical-align:middle;" src="images/eqn_0070.svg" alt=""/> = 0.0005 has been fixed in our implementation), the constant 1.134 is derived from the normal distribution for a sigma factor of 1.5 (as recommended in Huber's work), and the Winsorization function is defined as follows:</p>
<a id="__equation_19__"></a><div class="pidoc_equation"><img src="images/eqn_0071.svg" alt=""/><span class="pidoc_equation_number">[19]</span></div>
<p>Winsorized sigma clipping is an excellent pixel rejection algorithm for relatively large sets of 15 or more images. For more than 20 images, this algorithm yields significantly better results than sigma clipping consistently in all of our tests.</p>
</dd>
<dt>
<p><a id="averaged_sigma_clipping"></a> Averaged Sigma Clipping</p>
</dt>
<dd>
<p>Our implementation of averaged sigma clipping is a variant of the similar algorithm (AVSIGCLIP) from the <a href="http://iraf.noao.edu/scripts/irafhelp?imcombine" title="http://iraf.noao.edu/scripts/irafhelp?imcombine">imcombine task of IRAF</a>. This algorithm works in two phases. In the first phase, the gain of an ideal detector with zero readout noise is estimated for each pixel stack. The second phase is an iterative sigma clipping procedure, where the estimated gains are used to compute the dispersion of each pixel stack around the median. Dispersion is calculated based on Poisson statistics, under the assumption that the noise in the images is proportional to the square root of the mean pixel values:</p>
<a id="__equation_20__"></a><div class="pidoc_equation"><img src="images/eqn_0072.svg" alt=""/><span class="pidoc_equation_number">[20]</span></div>
<p>As we have implemented it, this algorithm works well for image sets of 10 or more images. The original implementation in IRAF works for smaller data sets because the proportionality between photons and pixel values, or the estimated sensor gain (the <img style="vertical-align:middle;" src="images/eqn_0073.svg" alt=""/> variable in the algorithm above) is calculated for each row of pixels. In our implementation the sensor gain is estimated separately for each pixel stack, in order to avoid visible differences between adjacent rows in the output image.</p>
</dd>
<dt>
<p><a id="linear_fit_clipping"></a> Linear Fit Clipping</p>
</dt>
<dd>
<p>The linear fit clipping algorithm fits the best possible straight line to the set of pixel values in each pixel stack. Line fitting is performed with a twofold optimization criterion: minimize average absolute deviation and maximize inliers. All pixels whose vertical distances to the fitted line are larger than a user-defined value in absolute deviation units are rejected. This algorithm has been created by PTeam member and principal PixInsight developer Juan Conejero:</p>
<a id="__equation_21__"></a><div class="pidoc_equation"><img src="images/eqn_0074.svg" alt=""/><span class="pidoc_equation_number">[21]</span></div>
<p>The FitLine function finds the two parameters of the line <img style="vertical-align:middle;" src="images/eqn_0075.svg" alt=""/> that minimizes average absolute deviation for an ordered set of points <img style="vertical-align:middle;" src="images/eqn_0076.svg" alt=""/>. In our implementation we have adapted a robust estimation algorithm from W. Press et al.,<sup><a href="#__reference_5__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 5]<br/>
William H. Press et al. (2007), <em>Numerical Recipes, The Art of Scientific Computing</em>, 3rd Ed., Cambridge University Press, &sect; 15.7.3, pp. 822&ndash;824">[5]</a></sup> but other robust methods are equally applicable, such as a least squares fit based on eigenvector evaluation.<sup><a href="#__reference_6__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 6]<br/>
Lawrence O'Gorman et al. (2009), <em>Practical Algorithms for Image Analysis</em>, 2nd Ed., Cambridge University Press, &sect; 5.5.2, pp. 214&ndash;215">[6]</a></sup> The LineClipping function is defined as</p>
<a id="__equation_22__"></a><div class="pidoc_equation"><img src="images/eqn_0077.svg" alt=""/><span class="pidoc_equation_number">[22]</span></div>
<p>Linear fit clipping is excellent for large sets of 25 or more images. The algorithm is robust both to outliers and to illumination differences among images of the integrated set, as happens in presence of sky gradients with differing spatial distributions and orientations. A byproduct of linear fit clipping is the slope of the fitted line for each pixel stack (the <img style="vertical-align:middle;" src="images/eqn_0078.svg" alt=""/> variable in the above algorithms), which provides accurate estimates of the illumination differences among the images integrated for each pixel. In the ImageIntegration tool we generate <a href="#rejection_maps"><em>slope map</em></a> images that gather this information.</p>

<div class="pidoc_figure">
<a id="__figure_5__"></a>
<p><span class="pidoc_figure_title">Figure 5 &mdash;</span>  <strong>Linear Fit Clipping Rejection</strong></p>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>
<img src="images/LinearFitClippingRejectionGraph.svg" alt=""/>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>
<p>Linear fit clipping working for the same sorted set of data samples shown in Figure <a href="#__figure_4__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Figure 4]">[4]</a> for <a href="#sigma_clipping">sigma clipping rejection</a>. As before, empty circles represent rejected pixels. The algorithm adapts much better to variations caused by gradients with differing orientations and distributions throughout the set of integrated images. In this example, robust line fitting allows working with a more restrictive clipping interval to reject true outliers while preserving more significant data.</p>
</div>
</dd>
<dt>
<p><a id="ccd_noise_model_clipping"></a> CCD Noise Model Clipping</p>
</dt>
<dd>
<p>We implement the same CCDCLIP algorithm of the <a href="http://iraf.noao.edu/scripts/irafhelp?imcombine" title="http://iraf.noao.edu/scripts/irafhelp?imcombine">imcombine task of IRAF</a>. To apply this pixel rejection algorithm one has to know the precise gain and readout noise parameters of the CCD sensor used to acquire the images. In addition, the images must preserve the original relation between pixel values and photons or electrons. The noise model is <sup><a href="#__reference_7__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 7]<br/>
R. A. Shaw, K. Horne (1992), <em>Noise Model-Based Cosmic Ray Rejection for WF/PC Images</em>, Astronomical Data Analysis Software and Systems I, A.S.P. Conference Series, Vol. 25, pp. 311&ndash;315">[7]</a></sup></p>
<a id="__equation_23__"></a><div class="pidoc_equation"><img src="images/eqn_0079.svg" alt=""/><span class="pidoc_equation_number">[23]</span></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0060.svg" alt=""/> is the pixel value in DN (DN stands for <em>data number</em>, or the value of a pixel stored in the raw CCD image), <img style="vertical-align:middle;" src="images/eqn_0080.svg" alt=""/> is the readout noise in DN, <img style="vertical-align:middle;" src="images/eqn_0081.svg" alt=""/> is the gain in DN/photon, and <img style="vertical-align:middle;" src="images/eqn_0007.svg" alt=""/> is the <em>scale noise</em>, also known as <em>sensitivity noise</em>, a dimensionless factor representing multiplicative noise, e.g. noise introduced by flat fielding. Usually the scale noise is unknown and hence set to zero. The constant term in <img style="vertical-align:middle;" src="images/eqn_0080.svg" alt=""/> represents additive noise, and the term proportional to the square root of the intensity value corresponds to <a href="http://en.wikipedia.org/wiki/Poisson_distribution" title="http://en.wikipedia.org/wiki/Poisson_distribution">Poisson noise</a>.</p>
<p>The algorithm is just a sigma clipping scheme where the CCD noise model is used to estimate the variability of the values in each pixel stack around the median:</p>
<a id="__equation_24__"></a><div class="pidoc_equation"><img src="images/eqn_0082.svg" alt=""/><span class="pidoc_equation_number">[24]</span></div>
</dd>
</dl>

</div>

<div class="pidoc_subsection" id="__Description_:_Image_Normalization__">
   <h4 class="pidoc_subsectionTitle">1.4&emsp;Image Normalization</h4>
<p>The normalization process modifies the distribution of pixel values of each input image to make the whole data set <em>statistically compatible</em>. When a set of two or more images are normalized, statistical moments such as the mean and the variance are quantitatively comparable between any pair of images from the set. Put in simpler terms, normalization allows us to compare the histograms of all of the images, ruling out differences in signal strength and mean background values. Normalization is an absolutely necessary previous step to pixel rejection. If the images are not normalized, any pixel rejection scheme will yield meaningless results because it will work by comparing incongruent data samples (e.g., pixels from the background on an image could be compared with pixels from significant objects on another image). In the ImageIntegration tool we have implemented two separate and independent image normalization procedures for pixel rejection and image combination, as both tasks have different requirements in their statistical interpretation of the data.</p>

<div class="pidoc_figure">
<a id="__figure_6__"></a>
<p><span class="pidoc_figure_title">Figure 6 &mdash;</span>  <strong>Image Normalization Example</strong></p>

<div class="pidoc_group" style="float:left;margin-right:10px;margin-bottom:10px;">
<img src="images/NormalizationExampleImage1.png" alt=""/>
<p><strong>a</strong></p>
</div>

<div class="pidoc_group">
<img src="images/NormalizationExampleImage1Histogram.png" alt=""/>
<p><strong>b</strong></p>
</div>

<div class="pidoc_group" style="float:left;margin-right:10px;margin-bottom:10px;">
<img src="images/NormalizationExampleImage2.png" alt=""/>
<p><strong>c</strong></p>
</div>

<div class="pidoc_group">
<img src="images/NormalizationExampleImage2Histogram.png" alt=""/>
<p><strong>d</strong></p>
</div>

<div class="pidoc_group" style="float:left;margin-right:10px;margin-bottom:10px;">
<img src="images/NormalizationExampleImage2Normalized.png" alt=""/>
<p><strong>e</strong></p>
</div>

<div class="pidoc_group">
<img src="images/NormalizationExampleImage2NormalizedHistogram.png" alt=""/>
<p><strong>f</strong></p>
</div>
<p>Two raw CCD images of the Cone nebula region, <strong>a</strong> and <strong>c,</strong> and their histograms <strong>b</strong> and <strong>d,</strong> respectively. Both original images are linear; they are represented here stretched nonlinearly to make them visible. For the same reason, the histograms have been enlarged 64:1 horizontally, so they cover a small initial section of 1/64 of the available numeric range. These images are not compatible statistically, as becomes evident by simply comparing the histograms: the positions and widths of both histogram peaks are very different. For example, mean background pixel values in the first image have values that are typical of bright nebulae in the second image. In <strong>e</strong> and <strong>f</strong> we have the second image (<strong>c</strong>) and its resulting histogram, respectively, after normalizing it to match the first image (<strong>a</strong>) by applying equations <a href="#__equation_33__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 33]<br/>
<img src='images/eqn_0126.svg' alt=''/>">[33]</a> and <a href="#__equation_34__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 34]<br/>
<img src='images/eqn_0131.svg' alt=''/>">[34]</a>. Images courtesy of Oriol Lehmkuhl and Ivette Rodr√≠guez.</p>
</div>
<p><a id="scale_estimators"></a></p>
<p><a id="rejection_normalization"></a></p>
<p><a id="output_normalization"></a></p>
<div class="pidoc_subsection" id="__Description_:_Image_Normalization_:_Scale_and_Location_Estimators__">
   <h5 class="pidoc_subsectionTitle">1.4.1&emsp;Scale and Location Estimators</h5>
<p>Statistical estimates of <a href="http://en.wikipedia.org/wiki/Central_tendency" title="http://en.wikipedia.org/wiki/Central_tendency">location</a> (or central tendency) and <a href="http://en.wikipedia.org/wiki/Statistical_dispersion" title="http://en.wikipedia.org/wiki/Statistical_dispersion">scale</a> (dispersion, or variability) play an essential role in the image integration task. For example, comparisons of unscaled noise estimates from different images are meaningless. Consider the two linear images, their histograms and noise estimates, shown in Figure <a href="#__figure_7__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Figure 7]">[7]</a>.</p>
<p>From the MRS Gaussian noise estimates,<sup><a href="#__reference_1__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 1]<br/>
Jean-Luc Starck and Fionn Murtagh (1998), <em>Automatic Noise Estimation from the Multiresolution Support</em>, Publications of the Royal Astronomical Society of the Pacific, vol. 110, pp. 193&ndash;199">[1]</a></sup> the bottom image seems to be about three times less noisy than the top image, so it should be weighted much more than the top image for integration (about 11 times more in a mean square error minimization scheme). Doesn't this seem to be contradictory to the visual appearance of both images? In fact, the bottom image is just a duplicate of the top image, multiplied by 0.3. So <em>both images have exactly the same signal and noise components</em>, because other than the applied scaling operation, they are identical.</p>

<div class="pidoc_figure">
<a id="__figure_7__"></a>
<p><span class="pidoc_figure_title">Figure 7 &mdash;</span>  <strong>Different Scales, Different Images?</strong></p>
<img style="float:left;margin-right:10px;" src="images/UnscaledNoiseExample1_img.jpg" alt=""/>
<img src="images/UnscaledNoiseExample1_hist.png" alt=""/>
<p><br/>
 <img style="vertical-align:middle;" src="images/eqn_0083.svg" alt=""/> <br class="pidoc_clearfix"/></p>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>
<img style="float:left;margin-right:10px;" src="images/UnscaledNoiseExample2_img.jpg" alt=""/>
<img src="images/UnscaledNoiseExample2_hist.png" alt=""/>
<p><br/>
 <img style="vertical-align:middle;" src="images/eqn_0084.svg" alt=""/> <br class="pidoc_clearfix"/></p>
<p>Two registered images of the same region of the sky, shown with adaptive nonlinear stretch functions applied (<a href="../../tools/ScreenTransferFunction/ScreenTransferFunction.html" title="../../tools/ScreenTransferFunction/ScreenTransferFunction.html">STF AutoStretch</a> functions in PixInsight). Each image is accompanied by its histogram and MRS Gaussian noise estimate. The different image scales (in the statistical sense, not geometrical) are self-evident from the histogram, although not visually because of the applied adaptive stretch functions.</p>
</div>
<p>When integrating real images, similar situations to the one described above happen naturally due to different exposure times, sky conditions, sensor temperatures, and other acquisition factors. To compare noise estimates between different images, one has to take into account not only the noise values, but also the scaling factors that must be applied to make the noise estimates statistically compatible. Besides noise estimation (and its associated image weighting criterion), pixel rejection also depends critically on estimators of location and scale. For example, with the images shown above, a pixel pertaining to the sky background in the top image would be ranked the same as a pixel on a relatively bright area in the bottom image, probably pertaining to a moderately bright star (compare the histograms to understand why this would happen).</p>
<p>With the sole exception of the <a href="#ikss_estimator">IKSS estimator</a>, which we'll see below, the <a href="http://en.wikipedia.org/wiki/Median" title="http://en.wikipedia.org/wiki/Median">median</a> is used as an estimator of location in the current versions of the ImageIntegration tool. The median is a <a href="http://cgm.cs.mcgill.ca/~athens/Geometric-Estimators/location.html" title="http://cgm.cs.mcgill.ca/~athens/Geometric-Estimators/location.html">robust estimator of location</a> that works remarkably well for linear images because the typical distribution of linear pixel values has a very strong central tendency. In other words, the main histogram peak of a linear image is clearly unique and prominent. The choice of a scale estimator is more difficult, and can have an impact on the optimization of the whole image integration process. In its current versions, ImageIntegration implements seven estimators of scale that we'll describe summarily below.</p>

<dl class="pidoc_list">
<dt>
<p><a id="avgdev_estimator"></a> Trimmed Average Absolute Deviation from the Median</p>
</dt>
<dd>
<p>The <a href="https://en.wikipedia.org/wiki/Absolute_deviation" title="https://en.wikipedia.org/wiki/Absolute_deviation">average absolute deviation</a> from the median has been the default scale estimator used in versions of the ImageIntegration tool released before mid-2013. For a sample <img style="vertical-align:middle;" src="images/eqn_0085.svg" alt=""/>,</p>
<a id="__equation_25__"></a><div class="pidoc_equation"><img src="images/eqn_0086.svg" alt=""/><span class="pidoc_equation_number">[25]</span></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0087.svg" alt=""/> is the sample median. As implemented in the ImageIntegration tool, the average absolute deviation is robustified by trimming all pixel samples outside the [0.00002,0.99998] range, which excludes cold and hot pixels, as well as most saturated pixels and bright spurious features (cosmics, etc). Yet this is a nonrobust estimator&mdash;its <a href="http://en.wiktionary.org/wiki/breakdown_point" title="http://en.wiktionary.org/wiki/breakdown_point">finite sample breakdown point</a> is zero&mdash;, so its use has to be questioned. The average absolute deviation has two important advantages though: its <a href="http://en.wikipedia.org/wiki/Efficient_estimator" title="http://en.wikipedia.org/wiki/Efficient_estimator">efficiency</a> is very high (88% for a normal distribution), and it is also a rather <a href="http://en.wikipedia.org/wiki/Sufficient_statistic" title="http://en.wikipedia.org/wiki/Sufficient_statistic">sufficient estimator</a>. Sufficiency of a statistical estimator refers to its ability to use all of the available sampled data to estimate its corresponding parameter. This explains why the average absolute deviation still works very well in some cases, and why it has been working reasonably well in general, until we have implemented the current set of robust scale estimators.</p>
</dd>
<dt>
<p><a id="mad_estimator"></a> Median Absolute Deviation from the Median (MAD)</p>
</dt>
<dd>
<p>The MAD estimator is the median of the set of absolute differences between the sample values and the sample median:</p>
<a id="__equation_26__"></a><div class="pidoc_equation"><img src="images/eqn_0088.svg" alt=""/><span class="pidoc_equation_number">[26]</span></div>
<p>MAD is a very robust estimator of scale. It has the best possible breakdown point (50%), but its <a href="http://en.wikipedia.org/wiki/Efficient_estimator" title="http://en.wikipedia.org/wiki/Efficient_estimator">efficiency</a> for a normal distribution is rather low (37%). MAD tends to work better for images with large background areas. Note that the word &quot;background&quot; here is being used with a purely statistical meaning; it can be the sky but also a dominant background nebula, for example. For more &quot;busy&quot; images, MAD tends to work worse because of its poor sufficiency.</p>
</dd>
<dt>
<p><a id="bwmv_estimator"></a> Biweight Midvariance</p>
</dt>
<dd>
<p>The square root of the biweight midvariance <sup><a href="#__reference_14__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 14]<br/>
Rand R. Wilcox (2012), <em>Introduction to Robust Estimation and Hypothesis Testing, 3rd Edition</em>, Elsevier Inc., &sect; 3.12.">[14]</a></sup> is a robust estimator of scale with a 50% breakdown point (the best one possible) and high efficiency with respect to several distributions (about 86%). The biweight midvariance is calculated as follows. For each sample value <img style="vertical-align:middle;" src="images/eqn_0089.svg" alt=""/> let</p>
<a id="__equation_27__"></a><div class="pidoc_equation"><img src="images/eqn_0090.svg" alt=""/><span class="pidoc_equation_number">[27]</span></div>
<p>and let the <em>indicator function</em> be defined as</p>
<div class="pidoc_equation"><img src="images/eqn_0091.svg" alt=""/></div>
<p>for <img style="vertical-align:middle;" src="images/eqn_0092.svg" alt=""/>. The biweight midvariance is then given by</p>
<a id="__equation_28__"></a><div class="pidoc_equation"><img src="images/eqn_0093.svg" alt=""/><span class="pidoc_equation_number">[28]</span></div>
<p>whose square root is a robust and efficient estimator of scale. For a detailed explanation of the reasons to use the constant 9 in equation <a href="#__equation_27__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 27]<br/>
<img src='images/eqn_0090.svg' alt=''/>">[27]</a>, see Wilcox (2012). <sup><a href="#__reference_14__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 14]<br/>
Rand R. Wilcox (2012), <em>Introduction to Robust Estimation and Hypothesis Testing, 3rd Edition</em>, Elsevier Inc., &sect; 3.12.">[14]</a></sup></p>
</dd>
<dt>
<p><a id="pbmv_estimator"></a> Percentage Bend Midvariance</p>
</dt>
<dd>
<p>The square root of the percentage bend midvariance <sup><a href="#__reference_14__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 14]<br/>
Rand R. Wilcox (2012), <em>Introduction to Robust Estimation and Hypothesis Testing, 3rd Edition</em>, Elsevier Inc., &sect; 3.12.">[14]</a></sup> is another robust estimator of scale. The percentage bend midvariance is interesting because it allows varying its efficiency and resistance properties, like a sort of <em>programmable</em> scale estimator in terms of efficiency/sufficiency versus robustness. Let</p>
<div class="pidoc_equation"><img src="images/eqn_0094.svg" alt=""/></div>
<p>Define the set <img style="vertical-align:middle;" src="images/eqn_0095.svg" alt=""/> of absolute differences from the sample median, and sort it in ascending order, so that we have <img style="vertical-align:middle;" src="images/eqn_0096.svg" alt=""/>. Define</p>
<div class="pidoc_equation"><img src="images/eqn_0097.svg" alt=""/></div>
<p>where the denominator is an estimate of the <img style="vertical-align:middle;" src="images/eqn_0098.svg" alt=""/> quantile of the distribution of <img style="vertical-align:middle;" src="images/eqn_0099.svg" alt=""/> Now the percentage bend midvariance is given by</p>
<a id="__equation_29__"></a><div class="pidoc_equation"><img src="images/eqn_0100.svg" alt=""/><span class="pidoc_equation_number">[29]</span></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0101.svg" alt=""/> is the same indicator function defined above for the biweight midvariance, and</p>
<div class="pidoc_equation"><img src="images/eqn_0102.svg" alt=""/></div>
<p>The <img style="vertical-align:middle;" src="images/eqn_0103.svg" alt=""/> parameter can be used to change the tradeoff between robustness and efficiency. The lower the value of <img style="vertical-align:middle;" src="images/eqn_0104.svg" alt=""/> the higher the efficiency, at the cost of a lower resistance. For the image integration task we have fixed <img style="vertical-align:middle;" src="images/eqn_0105.svg" alt=""/>, as recommended by Wilcox (2012) for a general purpose estimator. <sup><a href="#__reference_14__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 14]<br/>
Rand R. Wilcox (2012), <em>Introduction to Robust Estimation and Hypothesis Testing, 3rd Edition</em>, Elsevier Inc., &sect; 3.12.">[14]</a></sup> With this value we have a Gaussian efficiency of about a 67% and a breakdown point of 0.2.</p>
</dd>
<dt>
<p><a id="sn_and_qn_estimators"></a> Sn and Qn Estimators of Rousseeuw and Croux</p>
</dt>
<dd>
<p>The average deviation, MAD, biweight and bend midvariance estimators measure the variability of pixel sample values around the median. This makes sense for deep-sky images because the median closely represents the mean background of the image in most cases. However, these estimators work under the assumption that variations are symmetric with respect to the central value, which may not be quite true in many cases. The Sn and Qn scale estimators of Rousseeuw and Croux <sup><a href="#__reference_15__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 15]<br/>
P.J. Rousseeuw and C. Croux (1993), <em>Alternatives to the Median Absolute Deviation</em>, Journal of the American Statistical Association, Vol. 88, pp. 1273&ndash;1283">[15]</a></sup> don't measure dispersion around a central value. They evaluate dispersion based on differences between pairs of data points, which makes them robust to asymmetric and skewed distributions. Sn and Qn are as robust to outliers as MAD, but their Gaussian efficiencies are higher (58% and 87%, respectively). The drawback of these estimators is that they are computationally expensive, especially the Qn estimator.</p>
<p>The Sn estimator is defined as</p>
<a id="__equation_30__"></a><div class="pidoc_equation"><img src="images/eqn_0106.svg" alt=""/><span class="pidoc_equation_number">[30]</span></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0107.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0108.svg" alt=""/> are the order statistics of rank <img style="vertical-align:middle;" src="images/eqn_0109.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0110.svg" alt=""/>, respectively.</p>
<p>The Qn estimator can be defined as</p>
<a id="__equation_31__"></a><div class="pidoc_equation"><img src="images/eqn_0111.svg" alt=""/><span class="pidoc_equation_number">[31]</span></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0112.svg" alt=""/>, and the expression <img style="vertical-align:middle;" src="images/eqn_0113.svg" alt=""/> represents the <img style="vertical-align:middle;" src="images/eqn_0114.svg" alt=""/> quantile of the set <img style="vertical-align:middle;" src="images/eqn_0115.svg" alt=""/>. In other words, Qn is an estimate of the <img style="vertical-align:middle;" src="images/eqn_0114.svg" alt=""/> order statistic of the set of <img style="vertical-align:middle;" src="images/eqn_0116.svg" alt=""/> interpoint distances.</p>
<p>We must point out that the Sn and Qn estimators don't depend on any location estimate, as is the case for the rest of scale estimators (which use the sample median as an estimator of central tendency). Since Sn and Qn compute differences between data samples, they can provide more reliable results for skewed and asymmetric distributions.</p>
<p>By simple inspection of these algorithms it is clear that the complexity of a naive implementation is <img style="vertical-align:middle;" src="images/eqn_0117.svg" alt=""/>. Fortunately, alternative implementations exist with <img style="vertical-align:middle;" src="images/eqn_0118.svg" alt=""/> complexity. These variants, which we have adapted to the PixInsight/PCL platform, have been designed and implemented by the authors of the original algorithms. <sup><a href="#__reference_15__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 15]<br/>
P.J. Rousseeuw and C. Croux (1993), <em>Alternatives to the Median Absolute Deviation</em>, Journal of the American Statistical Association, Vol. 88, pp. 1273&ndash;1283">[15]</a></sup></p>
</dd>
<dt>
<p><a id="ikss_estimator"></a> Iterative K-sigma Estimator of Location and Scale (IKSS)</p>
</dt>
<dd>
<p>Given a data vector <img style="vertical-align:middle;" src="images/eqn_0119.svg" alt=""/> with values in the [0,1] range, the IKSS algorithm can be formalized as follows:</p>
<a id="__equation_32__"></a><div class="pidoc_equation"><img src="images/eqn_0120.svg" alt=""/><span class="pidoc_equation_number">[32]</span></div>
<p>The IKSS algorithm computes estimates of location and scale (the first and second elements, respectively, of the returned sets in the above algorithm) by evaluation of the biweight midvariance in an iterative k-sigma clipping scheme. The <img style="vertical-align:middle;" src="images/eqn_0121.svg" alt=""/> parameter is the fractional accuracy of the desired scale estimate. In our implementation we set <img style="vertical-align:middle;" src="images/eqn_0122.svg" alt=""/>, which normally requires from 4 to 10 iterations, depending on the distribution of pixel values of the image. The <img style="vertical-align:middle;" src="images/eqn_0123.svg" alt=""/> parameter is a safeguard to guarantee numerical stability in degenerate cases; usually we set it to twice the machine epsilon for IEEE 754 32-bit floating point: <img style="vertical-align:middle;" src="images/eqn_0124.svg" alt=""/>. Finally, the 0.991 constant makes the IKSS estimator consistent with the standard deviation of a normal distribution.</p>
<p>The IKSS estimator has a breakdown point of 0.5 and its Gaussian efficiency is 92%. Its resistance to outliers is much better than any of the rest of implemented estimators. In simulations with real images contaminated with synthetic impulsional noise of varying amplitudes, IKSS can tolerate in excess of a 50% of outliers without significant variations in the computed estimates. IKSS yields at least reasonably good results with the vast majority of deep-sky images, and therefore it is the default estimator of scale in the current versions of the ImageIntegration tool. When the IKSS estimator is selected, the IKSS estimate of location is also used instead of the median for all image normalization tasks.</p>
</dd>
</dl>

</div>

<div class="pidoc_subsection" id="__Description_:_Image_Normalization_:_Rejection_Normalization__">
   <h5 class="pidoc_subsectionTitle">1.4.2&emsp;Rejection Normalization</h5>
<p>Rejection normalization is applied to each input image just before the pixel rejection task. If the rejection and output normalization methods differ, the <img style="vertical-align:middle;" src="images/eqn_0125.svg" alt=""/> image generated in equations <a href="#__equation_33__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 33]<br/>
<img src='images/eqn_0126.svg' alt=''/>">[33]</a> and <a href="#__equation_35__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 35]<br/>
<img src='images/eqn_0134.svg' alt=''/>">[35]</a> is a temporary data structure used exclusively to decide which pixels are to be rejected as outliers; in such case it is disposed once the pixel rejection task has been completed.</p>

<dl class="pidoc_list">
<dt>
<p><a id="scaling_plus_zero_offset"></a> Scaling + Zero Offset</p>
</dt>
<dd>
<p>This normalization method matches mean background values and dispersion for all of the input images before pixel rejection. This is the default rejection normalization, which should <em>always</em> be applied to integrate calibrated raw images with reasonably flat illumination profiles. The scaling + zero offset normalization algorithm can be expressed as</p>
<a id="__equation_33__"></a><div class="pidoc_equation"><img src="images/eqn_0126.svg" alt=""/><span class="pidoc_equation_number">[33]</span></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0127.svg" alt=""/> is the working normalized image, <img style="vertical-align:middle;" src="images/eqn_0054.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0128.svg" alt=""/> are, respectively, the scaling factor and the location estimate for the <img style="vertical-align:middle;" src="images/eqn_0053.svg" alt=""/> input image, and <img style="vertical-align:middle;" src="images/eqn_0129.svg" alt=""/> is the location estimate for the reference image. Conventionally, the first image in the input set, namely <img style="vertical-align:middle;" src="images/eqn_0130.svg" alt=""/>, is taken as the reference image for normalization and image weighting tasks.</p>
<p>The median of all pixels in an image is always used as the estimator of location, except when the IKSS scale estimator is used, in which case <img style="vertical-align:middle;" src="images/eqn_0128.svg" alt=""/> is the IKSS location estimate.</p>
<p>Scaling factors are computed by the expression</p>
<a id="__equation_34__"></a><div class="pidoc_equation"><img src="images/eqn_0131.svg" alt=""/><span class="pidoc_equation_number">[34]</span></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0132.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0133.svg" alt=""/> are, respectively, the scale estimates for the reference and the <img style="vertical-align:middle;" src="images/eqn_0053.svg" alt=""/> input images.</p>
</dd>
<dt>
<p><a id="flux_equalization"></a> Flux Equalization</p>
</dt>
<dd>
<p>This method simply matches the main histogram peaks of all images prior to pixel rejection. This is done by multiplication with the ratio of the reference location estimate to the location estimate of each integrated image:</p>
<a id="__equation_35__"></a><div class="pidoc_equation"><img src="images/eqn_0134.svg" alt=""/><span class="pidoc_equation_number">[35]</span></div>
<p>This is the method of choice for rejection with sky flat field frames, since in this case trying to match dispersion does not make sense because of the irregular illumination distribution. For the same reason, this method of rejection normalization can also be useful to integrate uncalibrated images, or images suffering from strong gradients due to vignetting or light pollution.</p>
</dd>
</dl>

</div>

<div class="pidoc_subsection" id="__Description_:_Image_Normalization_:_Output_Normalization__">
   <h5 class="pidoc_subsectionTitle">1.4.3&emsp;Output Normalization</h5>
<p>Output normalization is applied to all input images just before the image combination task, only to those pixels that have survived after the rejection task.</p>

<dl class="pidoc_list">
<dt>
<p><a id="additive_output_normalization"></a> Additive Normalization</p>
</dt>
<dd>
<p>This normalization method matches mean background values among all images prior to the pixel combination task. The algorithm can be represented as</p>
<a id="__equation_36__"></a><div class="pidoc_equation"><img src="images/eqn_0135.svg" alt=""/><span class="pidoc_equation_number">[36]</span></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0136.svg" alt=""/> symbolizes one of the images that will feed the image combination task, and <img style="vertical-align:middle;" src="images/eqn_0006.svg" alt=""/> represents the corresponding input image.</p>
</dd>
<dt>
<p><a id="multiplicative_output_normalization"></a> Multiplicative Normalization</p>
</dt>
<dd>
<p>This method also matches backgrounds, but instead of additive operations, it applies a normalization by division:</p>
<a id="__equation_37__"></a><div class="pidoc_equation"><img src="images/eqn_0137.svg" alt=""/><span class="pidoc_equation_number">[37]</span></div>
<p>Additive and multiplicative normalizations lead to similar results in general. However, multiplicative normalization <em>should</em> be used to integrate images that are to be further combined or applied by multiplication or division. This is especially important for integration of flat frames, since a master flat frame should not contain any additive terms.</p>
</dd>
<dt>
<p><a id="output_scaling"></a> Scaling</p>
</dt>
<dd>
<p>Scaling matches dispersion among the images. This can be seen as a sort of <em>automatic weighting</em> correction to integrate images with differing overall illumination. The equations are</p>
<a id="__equation_38__"></a><div class="pidoc_equation"><img src="images/eqn_0138.svg" alt=""/><span class="pidoc_equation_number">[38]</span></div>
<p>and</p>
<a id="__equation_39__"></a><div class="pidoc_equation"><img src="images/eqn_0139.svg" alt=""/><span class="pidoc_equation_number">[39]</span></div>
<p>respectively for additive and multiplicative normalization with scaling, where <img style="vertical-align:middle;" src="images/eqn_0054.svg" alt=""/> is the scaling factor given by Equation <a href="#__equation_34__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 34]<br/>
<img src='images/eqn_0131.svg' alt=''/>">[34]</a>. Scaled output normalization is the recommended option for integration of light frames. In general, scaled normalization will lead to higher signal-to-noise ratios in the integrated result.</p>
</dd>
</dl>

</div>

<div class="pidoc_subsection" id="__Description_:_Image_Normalization_:_Recommended_Normalization_Methods__">
   <h5 class="pidoc_subsectionTitle">1.4.4&emsp;Recommended Normalization Methods</h5>
<p>The following table summarizes the recommended normalization methods for master calibration and light frames. The recommendations for light frames are generally valid in most cases. If there are <em>very strong</em> sky gradients or similar illumination variations, and their intensities vary considerably among the images being integrated, simple flux equalization may be a better option for rejection normalization.</p>

<div style="text-align:center;">

<table class="pidoc_table" style="width:100%;">
<caption><a id="__table_1__"></a>
<span class="pidoc_table_title">Table 1 &mdash;  Recommended Normalization Methods</span></caption>
<tr>
<th>
<div style="text-align:center;">
<p>Frame type</p>
</div>
</th>
<th>
<div style="text-align:center;">
<p>Rejection Normalization</p>
</div>
</th>
<th>
<div style="text-align:center;">
<p>Output Normalization</p>
</div>
</th>
</tr>
<tr>
<td><p>Master Bias</p>
</td>
<td><p>No normalization</p>
</td>
<td><p>No normalization</p>
</td>
</tr>
<tr>
<td><p>Master Dark</p>
</td>
<td><p>No normalization</p>
</td>
<td><p>No normalization</p>
</td>
</tr>
<tr>
<td><p>Master Flat</p>
</td>
<td><p>Equalize Fluxes</p>
</td>
<td><p>Multiplicative</p>
</td>
</tr>
<tr>
<td><p>Light</p>
</td>
<td><p>Scale + Zero Offset</p>
</td>
<td><p>Additive with scaling</p>
</td>
</tr>
</table>

</div>
</div>

</div>

<div class="pidoc_subsection" id="__Description_:_Quality_Assessment__">
   <h4 class="pidoc_subsectionTitle">1.5&emsp;Quality Assessment</h4>
<p>When the <em>evaluate noise</em> option is selected, ImageIntegration performs a (scaled) noise evaluation task on the final integrated image, and compares the computed noise estimates with the original integrated frames in order to assess the quality of the integration. Without this final assessment, image integration is kind of a &quot;faith-based&quot; process, where one has no way to know if the achieved image is making justice to the data with the available resources. This is contrary to the general philosophy of PixInsight. Bear in mind that the result of integration is the very starting point of your image, so knowing how good is it is of crucial importance.</p>
<p>Previous versions of ImageIntegration (prior to versions released since mid-2013) attempted to provide estimates of the signal-to-noise ratio (SNR) improvement. We have seen that when we <a href="#average_combination">average</a> N images, we can expect an SNR improvement equal to the square root of N (Equation <a href="#__equation_6__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 6]<br/>
<img src='images/eqn_0022.svg' alt=''/>">[6]</a>). This is a theoretical upper limit, which we'll never achieve due to a number of adverse factors (we work with discrete signals, we reject some pixels, not all of the images have the same quality, etc). Unfortunately, estimating the relative SNR gain is not a trivial problem, and the routines implemented in previous versions of the ImageIntegration tool were not as accurate as desirable. In some cases we have seen reported improvements slightly greater than the theoretical limit, which doesn't contribute to the confidence on these reports. We definitely need more accuracy.</p>
<p>In the latest versions of ImageIntegration we no longer attempt to evaluate SNR increments. Instead, we provide accurate estimates of the <em>effective noise reduction function</em> (ENR):</p>
<a id="__equation_40__"></a><div class="pidoc_equation"><img src="images/eqn_0140.svg" alt=""/><span class="pidoc_equation_number">[40]</span></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0130.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0141.svg" alt=""/> are the reference and integrated images, respectively, <img style="vertical-align:middle;" src="images/eqn_0142.svg" alt=""/> terms are noise estimates, and the scaling factor is given by</p>
<a id="__equation_41__"></a><div class="pidoc_equation"><img src="images/eqn_0143.svg" alt=""/><span class="pidoc_equation_number">[41]</span></div>
<p>where <img style="vertical-align:middle;" src="images/eqn_0132.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0144.svg" alt=""/> are scale estimates for the reference and integrated images, respectively. As is customary in our implementation, we use the multiresolution support noise evaluation algorithm <sup><a href="#__reference_1__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 1]<br/>
Jean-Luc Starck and Fionn Murtagh (1998), <em>Automatic Noise Estimation from the Multiresolution Support</em>, Publications of the Royal Astronomical Society of the Pacific, vol. 110, pp. 193&ndash;199">[1]</a></sup> and the <a href="#ikss_estimator">IKSS estimator of scale</a>, respectively for <img style="vertical-align:middle;" src="images/eqn_0142.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0073.svg" alt=""/> estimates.</p>
<p>To understand how effective noise reduction works in practice, consider the images shown in Figure <a href="#__figure_8__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Figure 8]">[8]</a>.</p>

<div class="pidoc_figure">
<a id="__figure_8__"></a>
<p><span class="pidoc_figure_title">Figure 8 &mdash;</span>  <strong>SNR Improvement Example</strong></p>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>
<img style="float:left;margin-right:10px;" src="images/EffectiveNoiseReductionExample_1a.jpg" alt=""/>
<img style="float:left;" src="images/EffectiveNoiseReductionExample_1b.jpg" alt=""/>
<p><br class="pidoc_clearfix"/></p>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>
<p>A crop of a single reference frame (left) and of the result of the integration of 20 frames (right). Both linear images are being shown with adaptive screen stretch functions computed for the whole images (not for the crops shown).</p>
</div>
<p>The image to the left is a crop of the reference frame of an integration set of 20 images. The right hand image is the same crop on the integrated result. Both linear images (the <em>whole</em> images, not the crops) are being shown with automatic screen stretch functions applied (<a href="../../tools/ScreenTransferFunction/ScreenTransferFunction.html" title="../../tools/ScreenTransferFunction/ScreenTransferFunction.html">STF</a> AutoStretch). We know that we can expect a maximum SNR increment of 4.47 approximately (the square root of 20). The achieved improvement is self-evident by comparing the images: the integrated result is much smoother than the original, and the higher SNR is also evident from many features that are clearly visible after integration, but barely detectable or invisible on the original.</p>
<p>This is a purely qualitative evaluation. Let's go a step forward in our analysis, and apply automatic screen stretch functions just to the cropped images&mdash;not to the whole images as before. As you know, the STF AutoStretch function is adaptive in the sense that it computes histogram transformation parameters based on statistical properties of the image. The result can be seen on Figure <a href="#__figure_9__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Figure 9]">[9]</a>.</p>

<div class="pidoc_figure">
<a id="__figure_9__"></a>
<p><span class="pidoc_figure_title">Figure 9 &mdash;</span>  <strong>Effective Noise Reduction</strong></p>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>

<div class="pidoc_group" style="float:left;margin-right:10px;">
<img src="images/EffectiveNoiseReductionExample_2a.jpg" alt=""/>
<p><br/>
 <strong>a</strong></p>
</div>

<div class="pidoc_group" style="float:left;">
<img src="images/EffectiveNoiseReductionExample_2b.jpg" alt=""/>
<p><br/>
 <strong>b</strong></p>
</div>
<p><br class="pidoc_clearfix"/></p>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>
<p>The same images shown in Figure <a href="#__figure_8__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Figure 8]">[8]</a>, but with adaptive screen stretch functions computed for the crops, instead of for the whole images.</p>
</div>
<p>Clearly not as 'nice' as before. The integrated image still shows many dim stars that are barely visible on the original, but the background noise levels are now quite similar: the illusion of a smooth result has evanesced. To explain why this happens we need some quantitative analysis. If we compute robust estimates of dispersion and noise for these cropped images, we get the following values:</p>

<div style="text-align:center;">

<table class="pidoc_table" style="width:100%;">
<tr>
<th>
<div style="text-align:center;">
<p>Image</p>
</div>
</th>
<th>
<div style="text-align:center;">
<p>IKSS scale estimate</p>
</div>
</th>
<th>
<div style="text-align:center;">
<p>MRS noise estimate</p>
</div>
</th>
</tr>
<tr>
<td><p>Reference frame (Figure 9.a)</p>
</td>
<td><p><img style="vertical-align:middle;" src="images/eqn_0145.svg" alt=""/></p>
</td>
<td><p><img style="vertical-align:middle;" src="images/eqn_0146.svg" alt=""/></p>
</td>
</tr>
<tr>
<td><p>Integrated frame (Figure 9.b)</p>
</td>
<td><p><img style="vertical-align:middle;" src="images/eqn_0147.svg" alt=""/></p>
</td>
<td><p><img style="vertical-align:middle;" src="images/eqn_0148.svg" alt=""/></p>
</td>
</tr>
</table>

</div>
<p>The first thing to note is that the standard deviations of the noise are quite similar to their corresponding scale estimates for each image (more similar in the original crop). This happens because these crops are dominated by the background of the image, where the noise also dominates over the signal. Now let's scale the integrated noise estimate with respect to the original. Applying Equation <a href="#__equation_41__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 41]<br/>
<img src='images/eqn_0143.svg' alt=''/>">[41]</a>, the scaling factor of the integrated crop with respect to the original crop is</p>
<div class="pidoc_equation"><img src="images/eqn_0149.svg" alt=""/></div>
<p>Applying Equation <a href="#__equation_40__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 40]<br/>
<img src='images/eqn_0140.svg' alt=''/>">[40]</a>, we have:</p>
<div class="pidoc_equation"><img src="images/eqn_0150.svg" alt=""/></div>
<p>This represents only about a 14% noise reduction, which intuitively is in good agreement with the differences shown in Figure <a href="#__figure_9__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Figure 9]">[9]</a>. ImageIntegration has reported an effective noise reduction factor of 1.298 for the whole image in this case (integration of 20 images). Now you know why the noise in background areas is so difficult to remove, even after stacking a good bunch of images: as soon as you stretch the image, the noise comes back on low SNR areas. Now you know also why robust and accurate estimators of noise and scale are so important.</p>
<p>This is how the latest versions of ImageIntegration evaluate the quality of an integration process. Effective noise reduction evaluation is much more accurate and robust than the SNR increments reported by previous versions. Don't let the low figures discourage you: they don't represent the SNR improvement that your are achieving with your data, but the noise reduction achieved on low-signal regions, where noise estimates are very accurate and reliable as quality estimators. Your goal when integrating a set of light frames is to achieve the maximum possible noise reduction with the necessary rejection of outlier pixels.</p>
</div>

   </div>
</div>

<div class="pidoc_section" id="__Usage__">
   <h3 class="pidoc_sectionTitle">2&emsp;Usage</h3>
   <p class="pidoc_sectionToggleButton" onclick="pidoc_toggleSection( 'Usage', this );">[hide]</p>
   <div id="Usage">
<p><a id="format_hints"></a></p>
<p><a id="region_of_interest"></a></p>
<p><a id="rejection_maps"></a></p>
<p><a id="console_statistics"></a></p>
<p><a id="cache_management"></a></p>
<div class="pidoc_subsection" id="__Usage_:_Input_Images__">
   <h4 class="pidoc_subsectionTitle">2.1&emsp;Input Images</h4>
<img src="images/InputImages.png" alt=""/>
<p>Use these controls to define and manage a list of image files to be integrated. For best performance and optimal resource usage, you should always work, as far as possible, with file formats able to perform <em>incremental reading</em> operations. Incremental reading consists of loading images by successive strips of pixel rows. As of writing this documentation, only the FITS format supports this functionality.</p>
<p>An important task that should always be carried out <em>before</em> image registration and integration is analysis and evaluation of the quality of the data. We strongly recommend you use the <a href="../../scripts/SubframeSelector/SubframeSelector.html" title="../../scripts/SubframeSelector/SubframeSelector.html">SubframeSelector</a> script to perform all image grading and selection tasks with calibrated images as a batch process. This script allows you to qualify your images based on a variety of criteria, including noise estimates, FWHM and star eccentricity, among others, and arithmetic combinations of them. The <a href="../../tools/Blink/Blink.html" title="../../tools/Blink/Blink.html">Blink</a> tool is also invaluable for visual inspection and statistical analysis of sets of images.</p>

<dl class="pidoc_list">
<dt>
<p><a id="input_images_list"></a> Input images list</p>
</dt>
<dd>
<p>The largest control in this section is a list box with all the images currently selected for integration. The list will show full file paths or just file names, depending on the state of the <a href="#full_paths">Full Paths checkbox</a>. You must select at least three files. On this list you can:</p>

<ul class="pidoc_list">
<li>Double-click an item's file name or path to open it in PixInsight as a new image window.</li>
<li>Double-click a green checkmark icon to disable an item (double-click the red crossmark icon to enable it). Disabled files will not be integrated.</li>
<li>Mouse over an item to see its full file path as a tool tip window.</li>
</ul>

</dd>
<dt>
<p>Add Files</p>
</dt>
<dd>
<p>Click this button to open a file dialog where you can select new image files that will be appended to the current list of files to be integrated. Only files located in the local filesystem can be selected; the tool does not currently support remote files located on network devices. As noted at the beginning of this section, only file formats with incremental reading capabilities should be selected (e.g., FITS) for performance reasons, although the tool supports any installed file format.</p>
</dd>
<dt>
<p>Set Reference</p>
</dt>
<dd>
<p>The ImageIntegration tool uses a <em>reference image</em>. By convention, the first (enabled) file in the input list is the reference image. The reference image is used for:</p>

<ul class="pidoc_list">
<li><a href="#image_normalization">Image normalization</a>. All input images will be normalized (both pixel rejection normalization and output normalization) to be compatible with the statistical properties of the reference image.</li>
<li class="pidoc_spaced_list_item"><a href="#image_weighting">Image weighting</a>. All image weights will be computed relative to the reference image, which will be assigned unit weight by convention.</li>
<li class="pidoc_spaced_list_item"><a href="#quality_assessment">Quality assessment</a>. When the corresponding option is selected, effective noise reduction (ENR) estimates are computed relative to all integrated images, and the ENR estimate relative to the reference image, as well as the median ENR estimate, are provided.</li>
</ul>

<p>For optimum performance of the image weighting and pixel rejection tasks, the reference image should be chosen as one of the best frames in the integration set. As for image weighting, the choice of a reference image is theoretically irrelevant under ideal conditions, but given that we normally have to work under much-less-than-ideal conditions, these points can help you to select an optimal integration reference image:</p>

<ul class="pidoc_list">
<li>If there are varying gradients in the data set, select the image with the least/weakest gradients. Gradients complicate calculation of fundamental statistical properties such as scale and location.</li>
<li class="pidoc_spaced_list_item">Try to select the best image in terms of SNR. In general, this corresponds to the image with the least noise estimate.</li>
<li class="pidoc_spaced_list_item">Avoid selecting a reference image with strong artifacts, such as plane and satellite trails, etc.</li>
</ul>

<p>As noted at the beginning of this section, we recommend the <a href="../../scripts/SubframeSelector/SubframeSelector.html" title="../../scripts/SubframeSelector/SubframeSelector.html">SubframeSelector</a> script and the <a href="../../tools/Blink/Blink.html" title="../../tools/Blink/Blink.html">Blink</a> tool for all image grading tasks.</p>
</dd>
<dt>
<p>Select All</p>
</dt>
<dd>
<p>Click this button to select all the files in the current list of input images.</p>
</dd>
<dt>
<p>Invert Selection</p>
</dt>
<dd>
<p>Click this button to invert the current selection in the list of input images.</p>
</dd>
<dt>
<p>Toggle Selected</p>
</dt>
<dd>
<p>Click this button to enable/disable the files currently selected in the list of input images.</p>
</dd>
<dt>
<p>Remove Selected</p>
</dt>
<dd>
<p>This button removes the selected files from the list of input images. This action cannot be undone.</p>
</dd>
<dt>
<p>Clear</p>
</dt>
<dd>
<p>Click to empty the list of input images. This action cannot be undone.</p>
</dd>
<dt>
<p><a id="full_paths"></a> Full Paths</p>
</dt>
<dd>
<p>When this option is selected, the list of input files will show the full absolute file paths of all selected images. When this option is disabled (default state), only file names will be shown, which simplifies visual inspection, and full file paths are shown as tool tip messages.</p>
</dd>
</dl>

</div>

<div class="pidoc_subsection" id="__Usage_:_Format_Hints__">
   <h4 class="pidoc_subsectionTitle">2.2&emsp;Format Hints</h4>
<img src="images/FormatHints.png" alt=""/>
<p>Format hints are small text strings that allow you to override global file format settings for image files used by specific processes. In the ImageIntegration tool, <em>input hints</em> change the way input images of some particular file formats are loaded during the integration process. There are no <em>output hints</em> in ImageIntegration, since this process does not write images to disk files.</p>
<p>Most standard file format modules support hints; each format supports a number of input and/or output hints that you can use for different purposes with tools that give you access to format hints. In the following tables we give you complete information on the input hints supported by the most important standard file formats in PixInsight.</p>

<table class="pidoc_table">
<caption><a id="__table_2__"></a>
<span class="pidoc_table_title">Table 2 &mdash;  FITS Input Hints</span></caption>
<tr>
<th><p>Hint</p>
</th>
<th><p>Purpose</p>
</th>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">lower-range</span></span> <em>n</em></p>
</td>
<td><p>Specifies the lower bound of the floating point input range.<br/>
 Example: <span class="pidoc_code">lower-range 0.001</span></p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">upper-range</span></span> <em>n</em></p>
</td>
<td><p>Specifies the upper bound of the floating point input range.<br/>
 Example: <span class="pidoc_code">upper-range 65535</span></p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">rescale</span></span></p>
</td>
<td><p>Forces rescaling of out-of-range floating point images to the floating point input range.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">rescale-out-of-range</span></span></p>
</td>
<td><p>A more explicit synonym to <span class="pidoc_code">rescale.</span></p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">truncate</span></span></p>
</td>
<td><p>Forces truncation of out-of-range floating point images to the floating point input range.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">truncate-out-of-range</span></span></p>
</td>
<td><p>A more explicit synonym to <span class="pidoc_code">truncate.</span></p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">ignore-out-of-range</span></span></p>
</td>
<td><p>Ignores out-of-range floating point pixel values. <strong>Attention: platform stability is not guaranteed if out-of-range pixels propagate from processing tools.</strong></p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">bottom-up</span></span></p>
</td>
<td><p>Sets the coordinate origin at the bottom left corner of each pixel matrix. Horizontal coordinates grow from left to right. Vertical coordinates grow from bottom to top.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">up-bottom</span></span></p>
</td>
<td><p>The inverse option to <span class="pidoc_code">bottom-up:</span> Sets the coordinate origin at the top left corner of each pixel matrix. Horizontal coordinates grow from left to right. Vertical coordinates grow from top to bottom.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">signed-is-physical</span></span></p>
</td>
<td><p>Specifies that signed integer images store physical data in the positive range [0,2<sup>n-1</sup>-1], where n is the number of bits per pixel. The negative part of the numeric range is not used, and the images are effectively represented with one bit less than the number of bits used to store each pixel in the image</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">signed-is-logical</span></span></p>
</td>
<td><p>Specifies that signed integer images store logical data in the full range [-2<sup>n-1</sup>,+2<sup>n-1</sup>-1].</p>
</td>
</tr>
</table>


<table class="pidoc_table">
<caption><a id="__table_3__"></a>
<span class="pidoc_table_title">Table 3 &mdash;  DSLR_RAW Input Hints</span></caption>
<tr>
<th><p>Hint</p>
</th>
<th><p>Purpose</p>
</th>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">raw</span></span></p>
</td>
<td><p>Enables a set of options to load pure raw images without interpolation, white balance or black pedestal subtraction. <span class="pidoc_code">raw</span> is equivalent to the following hints: <span class="pidoc_code">bayer-drizzle no-super-pixels no-cfa no-auto-white-balance no-camera-white-balance no-black-point-correction</span>.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">cfa</span></span></p>
</td>
<td><p>Load raw Bayer CFA image (no interpolation, no de-Bayering, monochrome image).</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">no-cfa</span></span></p>
</td>
<td><p>Load raw Bayer RGB image (no interpolation, no de-Bayering, RGB image).</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">bilinear</span></span></p>
</td>
<td><p>Use bilinear de-Bayering interpolation.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">fast</span></span></p>
</td>
<td><p>A synonym for <span class="pidoc_code">bilinear.</span></p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">vng</span></span></p>
</td>
<td><p>Use the Threshold-Based Variable Number of Gradients (VNG) interpolation algorithm.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">ppg</span></span></p>
</td>
<td><p>Use the Patterned Pixel Grouping (PPG) interpolation algorithm.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">ahd</span></span></p>
</td>
<td><p>Use the Adaptive Homogeneity-Directed (AHD) interpolation algorithm.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">interpolate-as-4-colors</span></span></p>
</td>
<td><p>Interpolate RGB as four colors.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">no-interpolate-as-4-colors</span></span></p>
</td>
<td><p>Interpolate RGB as three colors (two green pixels, one red and one blue).</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">auto-white-balance</span></span></p>
</td>
<td><p>Compute and apply an automatic white balance.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">no-auto-white-balance</span></span></p>
</td>
<td><p>Do not apply an automatic white balance.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">camera-white-balance</span></span></p>
</td>
<td><p>Apply a camera-defined white balance, if available.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">no-camera-white-balance</span></span></p>
</td>
<td><p>Do not apply a camera-defined white balance.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">super-pixels</span></span></p>
</td>
<td><p>Create super-pixels from the raw Bayer matrix (no interpolation).</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">no-super-pixels</span></span></p>
</td>
<td><p>Do not use the super-pixel de-Bayering method.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">bayer-drizzle</span></span></p>
</td>
<td><p>Do not interpolate the Bayer matrix. Load a RGB or CFA raw Bayer image.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">no-bayer-drizzle</span></span></p>
</td>
<td><p>Interpolate the Bayer matrix. Load an RGB color image.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">black-point-correction</span></span></p>
</td>
<td><p>Subtract <em>darkness level</em> pedestals.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">no-black-point-correction</span></span></p>
</td>
<td><p>Do not subtract black point pedestals. Load all images referred to a fixed zero black point value.</p>
</td>
</tr>
</table>


<table class="pidoc_table">
<caption><a id="__table_4__"></a>
<span class="pidoc_table_title">Table 4 &mdash;  TIFF Input Hints</span></caption>
<tr>
<th><p>Hint</p>
</th>
<th><p>Purpose</p>
</th>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">lower-range</span></span> <em>n</em></p>
</td>
<td><p>Specifies the lower bound of the floating point input range.<br/>
 Example: <span class="pidoc_code">lower-range 0.001</span></p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">upper-range</span></span> <em>n</em></p>
</td>
<td><p>Specifies the upper bound of the floating point input range.<br/>
 Example: <span class="pidoc_code">upper-range 65535</span></p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">rescale</span></span></p>
</td>
<td><p>Forces rescaling of out-of-range floating point images to the floating point input range.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">rescale-out-of-range</span></span></p>
</td>
<td><p>A more explicit synonym to <span class="pidoc_code">rescale.</span></p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">truncate</span></span></p>
</td>
<td><p>Forces truncation of out-of-range floating point images to the floating point input range.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">truncate-out-of-range</span></span></p>
</td>
<td><p>A more explicit synonym to <span class="pidoc_code">truncate.</span></p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">ignore-out-of-range</span></span></p>
</td>
<td><p>Ignores out-of-range floating point pixel values. <strong>Attention: platform stability is not guaranteed if out-of-range pixels propagate from processing tools.</strong></p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">strict</span></span></p>
</td>
<td><p>Be strict with TIFF tags, tag values and image properties.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">permissive</span></span></p>
</td>
<td><p>Be tolerant with slightly incorrect images and images that don't comply to strict TIFF format specifications.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">show-warnings</span></span></p>
</td>
<td><p>Show TIFF format warnings on the console and/or message boxes.</p>
</td>
</tr>
<tr>
<td><p><span class="pidoc_code"><span class="pidoc_nowrap">no-show-warnings</span></span></p>
</td>
<td><p>Never show warning messages.</p>
</td>
</tr>
</table>

<p>If multiple input hints are specified, they must be separated by spaces.</p>
</div>

<div class="pidoc_subsection" id="__Usage_:_Image_Integration__">
   <h4 class="pidoc_subsectionTitle">2.3&emsp;Image Integration</h4>
<img src="images/ImageIntegrationParameters.png" alt=""/>
<p>This section allows you to control the image combination task.</p>

<dl class="pidoc_list">
<dt>
<p>Combination</p>
</dt>
<dd>
<p>This list box allows you to select the <a href="#image_combination">image combination</a> operation:</p>

<ul class="pidoc_list">
<li><strong><a href="#average_combination">Average.</a></strong> The output integrated image is the pixel-by-pixel mean of all input images. Provides the highest signal-to-noise ratio in the integrated result.</li>
<li class="pidoc_spaced_list_item"><strong><a href="#median_combination">Median.</a></strong> The output integrated image is the pixel-by-pixel median of all input images. Provides robust implicit rejection of outliers, but at the cost of about a 20% signal loss.</li>
<li class="pidoc_spaced_list_item"><strong>Minimum.</strong> Each pixel of the output integrated image is the minimum value of the corresponding pixels from all input images. This mode is only useful for special purposes; it should not be used for normal image stacking operations.</li>
<li class="pidoc_spaced_list_item"><strong>Maximum.</strong> Each pixel of the output integrated image is the maximum value of the corresponding pixels from all input images. This mode is only useful for special purposes; it should not be used for normal image stacking operations.</li>
</ul>

</dd>
<dt>
<p>Normalization</p>
</dt>
<dd>
<p>Selects the <a href="#output_normalization">output normalization</a> method used for the image combination task:</p>

<ul class="pidoc_list">
<li><strong>No normalization.</strong> If this option is selected, the images won't be normalized prior to combination. This is useful when there are <em>pedestals</em> in the input images that must be preserved, as happens when integrating master bias and master dark frames.<sup><a href="#__reference_17__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 17]<br/>
Vicent Peris (2010), <a href='http://pixinsight.com/tutorials/master-frames/en.html' title='http://pixinsight.com/tutorials/master-frames/en.html'><em>Master Calibration Frames: Acquisition and Processing</em></a>, tutorial.">[17]</a></sup> Normally this option should not be selected for integration of flat and light frames.</li>
<li class="pidoc_spaced_list_item"><strong><a href="#additive_output_normalization">Additive.</a></strong> Additive operations will be applied to match mean background values.</li>
<li class="pidoc_spaced_list_item"><strong><a href="#multiplicative_output_normalization">Multiplicative.</a></strong> Mean background values will be matched by division. This option <em>must</em> be used to integrate master flat frames. It can also be used to generate images that are to be further combined by multiplication or division.</li>
<li class="pidoc_spaced_list_item"><strong><a href="#output_scaling">Additive + scaling.</a></strong> Along with additive background matching, the images will be scaled to match dispersion. This is the default option, which should normally be used to integrate light frames.</li>
<li class="pidoc_spaced_list_item"><strong>Multiplicative + scaling.</strong> Along with background matching by division, the images will be scaled to match dispersion. This option can be used to generate images that are to be further combined by multiplication or division.</li>
</ul>

</dd>
<dt>
<p>Weights</p>
</dt>
<dd>
<p>Selects an <a href="#image_weighting">image weighting</a> method:</p>

<ul class="pidoc_list">
<li><strong>Don't care.</strong> If this option is selected, no weighting will be applied and all input images will be combined directly. This mode should be selected to integrate master bias, dark and flat frames. For science frames, weighting should <em>always</em> be enabled.</li>
<li class="pidoc_spaced_list_item"><strong>Exposure time.</strong> Will weight the input images by their relative exposures. Exposure times will be retrieved from standard EXPTIME and EXPOSURE FITS keywords (in that order).</li>
<li class="pidoc_spaced_list_item"><strong><a href="#noise_evaluation_weighting">Noise evaluation.</a></strong> Uses multiscale noise evaluation techniques <sup><a href="#__reference_1__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 1]<br/>
Jean-Luc Starck and Fionn Murtagh (1998), <em>Automatic Noise Estimation from the Multiresolution Support</em>, Publications of the Royal Astronomical Society of the Pacific, vol. 110, pp. 193&ndash;199">[1]</a></sup> <sup><a href="#__reference_2__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 2]<br/>
Jean-Luc Starck and Fionn Murtagh (2002), <em>Astronomical Image and Data Analysis</em>, Springer, pp. 36&ndash;39">[2]</a></sup> to optimize the integration for mean square error minimization. This is the most accurate and robust image weighting algorithm currently available in the ImageIntegration tool, and is therefore the default option.</li>
<li class="pidoc_spaced_list_item"><strong>Average signal strength.</strong> Estimates relative exposures from statistical properties of the images. This method will <em>not</em> work <em>at all</em> if some images have additive illumination variations, such as sky gradients.</li>
<li class="pidoc_spaced_list_item"><strong>Median value.</strong> Weights the input images by their median pixel sample values, relative to the reference image.</li>
<li class="pidoc_spaced_list_item"><strong>Average value.</strong> Weights the input images by their mean pixel sample values, relative to the reference image.</li>
<li class="pidoc_spaced_list_item"><strong>FITS keyword.</strong> Uses the values associated with a custom FITS keyword to retrieve image weights, which must be specified in the corresponding <em>weight keyword</em> input field (see below). The specified keyword must be present in all input images and its value must be of a numeric type.</li>
</ul>

</dd>
<dt>
<p>Weight keyword</p>
</dt>
<dd>
<p>Custom FITS keyword to retrieve image weights. This is the name of a FITS keyword that will be used to retrieve image weights, if the <em>FITS keyword</em> option has been selected as the weighting criterion (see above).</p>
</dd>
<dt>
<p>Scale estimator</p>
</dt>
<dd>
<p>Selects a statistical <a href="#scale_estimators">estimator of scale</a>:</p>

<ul class="pidoc_list">
<li><a href="#avgdev_estimator"><strong>Average absolute deviation from the median.</strong></a> This has been the default scale estimator used in versions of the ImageIntegration tool released before mid-2013. It is robustified by trimming all pixel samples outside the [0.00002,0.99998] range, which excludes cold and hot pixels, as well as most saturated pixels and bright spurious features (cosmics, etc). Yet this is a nonrobust estimator (its breakdown point is zero), but on the other hand it is a very efficient and sufficient estimator.</li>
<li class="pidoc_spaced_list_item"><a href="#mad_estimator"><strong>Median absolute deviation from the median (MAD).</strong></a> MAD is a very robust estimator of scale. Although it has the best possible breakdown point (50%), its efficiency for a normal distribution is rather low (37%). It tends to work better for images with large background areas&mdash;we are using the term background here with a purely statistical meaning; it can be the sky but also a dominant background nebula, for example.</li>
<li class="pidoc_spaced_list_item"><a href="#bwmv_estimator"><strong>Biweight midvariance.</strong></a> The square root of the biweight midvariance is a robust estimator of scale with a 50% breakdown point (as good as MAD) and high efficiency with respect to several distributions (about 86%).</li>
<li class="pidoc_spaced_list_item"><a href="#pbmv_estimator"><strong>Percentage bend midvariance.</strong></a> The square root of the percentage bend midvariance is another robust estimator of scale with high efficiency (67%) and good resistance to outliers.</li>
<li class="pidoc_spaced_list_item"><a href="#sn_and_qn_estimators"><strong>Sn and Qn estimators of Rousseeuw and Croux.</strong></a> The average deviation, MAD, biweight and bend midvariance estimators measure the variability of pixel sample values around the median. This makes sense for deep-sky images because the median closely represents the mean background of the image in most cases. However, these estimators work under the assumption that variations are symmetric with respect to the central value, which may not be quite true in many cases. The Sn and Qn scale estimators don't measure dispersion around a central value. They evaluate dispersion based on differences between pairs of data points, which makes them robust to asymmetric and skewed distributions. Sn and Qn are as robust to outliers as MAD, but their Gaussian efficiencies are higher (58% and 87%, respectively). The drawback of these estimators is that they are computationally expensive, especially the Qn estimator.</li>
<li class="pidoc_spaced_list_item"><a href="#ikss_estimator"><strong>Iterative k-sigma estimator of location and scale (IKSS).</strong></a> This is a robust sigma-clipping routine based on the biweight midvariance. The idea is similar to M-estimators of location. From our tests, IKSS is as robust to outliers as MAD, and its Gaussian efficiency exceeds the 90%. This is the default estimator of scale in current versions of the ImageIntegration tool.</li>
</ul>

<p>The selected scale estimator will be used in all subtasks requiring image scaling factors. This includes the noise evaluation image weighting routine, normalization in all pixel rejection algorithms, output normalization when a scaling option is selected, and the final quality assessment step.</p>
<p>In general, the default IKSS estimator of scale works almost optimally in most cases. If you really want to get the most out of your data, you should at least make some tries with IKSS, MAD and average absolute deviation. The goal is to maximize effective noise reduction while achieving a good outlier rejection.</p>
</dd>
<dt>
<p>Ignore noise keywords</p>
</dt>
<dd>
<p>If this option is disabled (default state), ImageIntegration will retrieve noise estimates from NOISExx FITS keywords, when available. If this option is enabled, existing noise keywords will be ignored and noise estimates will be calculated or retrieved from cached data. Use this option if you don't trust noise estimates stored in FITS header keywords for some reason.</p>
<p>Note that the <a href="../../tools/ImageCalibration/ImageCalibration.html" title="../../tools/ImageCalibration/ImageCalibration.html">ImageCalibration</a> and <a href="../../tools/Debayer/Debayer.html" title="../../tools/Debayer/Debayer.html">Debayer</a> tools compute noise estimates and store them as FITS keywords by default. In general, you should not need to enable this option under normal working conditions.</p>
</dd>
<dt>
<p>Generate integrated image</p>
</dt>
<dd>
<p>When this option is selected, the result of the integration process will be generated in a new image window. This option should be enabled for normal use. If you disable it, the integrated image won't be generated at the end of the process. You can disable this option to save a relatively modest amount of computation time and resources while you are trying out rejection parameters, since to evaluate the suitability of pixel rejection, you normally are only interested in <a href="#console_statistics">rejection statistics</a> and/or <a href="#rejection_maps">rejection maps</a>. This option is enabled by default.</p>
</dd>
<dt>
<p>Generate a 64-bit result image</p>
</dt>
<dd>
<p>If this is selected, ImageIntegration will generate the result image in <a href="http://en.wikipedia.org/wiki/IEEE_754-2008" title="http://en.wikipedia.org/wiki/IEEE_754-2008">IEEE 754</a> 64-bit floating point format (double precision). Otherwise the integration result will be generated in IEEE 754 32-bit floating point format (single precision), which is the default option. Even if the result is a single precision image, all intermediate calculations are performed using double precision internally.</p>
</dd>
<dt>
<p><a id="evaluate_noise"></a> Evaluate noise</p>
</dt>
<dd>
<p>This option enables the final <a href="#quality_assessment">quality assessment</a> task. ImageIntegration will compute estimates of noise and effective noise reduction at the end of the process. This is useful to compare the results of different integration procedures. For example, by comparing quality data you can know which image normalization and weighting criteria lead to the best result in terms of signal-to-noise ratio improvement. This option is enabled by default.</p>
</dd>
<dt>
<p>Close previous images</p>
</dt>
<dd>
<p>Select this option to close existing integration and rejection map images before running a new integration process. This is useful to avoid accumulation of multiple results on the workspace, when the same integration is being tested repeatedly.</p>
</dd>
<dt>
<p><a id="buffer_size"></a> Buffer size</p>
</dt>
<dd>
<p>Size of a <em>pixel row buffer</em> in mebibytes (MiB). This parameter defines the size of the working buffers used to read pixel rows. There is an independent buffer per input image. A reasonably large buffer size will improve performance by minimizing disk reading operations. The default value of 16 MiB is usually quite appropriate. Decrease this parameter if you experience out-of-memory errors during integration. This may be necessary for integration of large image sets on systems with low memory resources, especially on 32-bit operating systems. The minimum value is zero, which will force ImageIntegration to use a single row of pixels per input image.</p>
</dd>
<dt>
<p>Stack size</p>
</dt>
<dd>
<p>This is the size of the working integration stack structure in MiB. In general, the larger this parameter, the better the performance, especially on multiprocessor and multicore systems. The best performance is achieved when the whole set of integrated pixels can be loaded at once in the integration stack. For this to happen, the following conditions must hold:</p>

<ul class="pidoc_list">
<li><em>Buffer size</em> (see above) must be large enough as to allow loading an input file (in 32-bit floating point format) completely in a single file reading operation.</li>
<li class="pidoc_spaced_list_item"><em>Stack size</em> must be larger than or equal to W&times;H&times;(12&times;N + 4), where W and H are the image width and height in pixels, respectively, and N is the number of integrated images. For linear fit clipping rejection, replace 4 with 8 in the above equation. Note that this may require a large amount of RAM available for relatively large image sets. As an example, the default stack size of 1024 (1 GiB) is sufficient to integrate 20 2048&times;2048 monochrome images optimally with the default buffer size of 16 MiB. With a stack size of 4 GiB and a buffer size of 64 MiB you could integrate 20 4K&times;4K monochrome images with optimum performance on a 64-bit version of PixInsight.</li>
</ul>

</dd>
<dt>
<p><a id="use_file_cache"></a> Use file cache</p>
</dt>
<dd>
<p>By default, ImageIntegration uses a dynamic cache of working image parameters, including pixel statistics and normalization data. This cache greatly improves performance when the same images are being integrated several times, for example to find optimal pixel rejection parameters. Disable this option if for some reason you don't want to use the cache. This will force recalculation of all statistical data required for normalization, which involves loading all integrated image files from disk. The file cache can also be <em>persistent</em> across PixInsight Core executions. The persistent cache and its options can be controlled with the <a href="#cache_management">Cache Preferences dialog</a>.</p>
</dd>
</dl>

</div>

<div class="pidoc_subsection" id="__Usage_:_Pixel_Rejection_1__">
   <h4 class="pidoc_subsectionTitle">2.4&emsp;Pixel Rejection (1)</h4>
<img src="images/PixelRejection1.png" alt=""/>
<p>In this section you can select general pixel rejection options:</p>

<dl class="pidoc_list">
<dt>
<p>Rejection algorithm</p>
</dt>
<dd>
<p>No rejection algorithm is selected by default in the ImageIntegration tool. This has been done intentionally to <em>encourage</em> the user to find the appropriate rejection method according to the number of input images and their conditions. The <a href="#pixel_rejection">available rejection algorithms</a> have already been described in detail; this is just a quick reference:</p>

<ul class="pidoc_list">
<li><a href="#minmax_clipping"><strong>Min/max.</strong></a> This method can be used to ensure rejection of extreme values. Min/max performs an unconditional rejection of a fixed number of pixels from each stack, without any statistical basis. Rejection methods based on robust statistics, such as percentile, Winsorized sigma clipping, linear fitting and averaged sigma clipping are in general preferable.</li>
<li class="pidoc_spaced_list_item"><a href="#percentile_clipping"><strong>Percentile clipping</strong></a> rejection is excellent to integrate reduced sets of images, such as 3 to 6 images. This is a single-pass algorithm that rejects pixels outside a fixed range of values relative to the median of each pixel stack.</li>
<li class="pidoc_spaced_list_item"><a href="#sigma_clipping"><strong>Sigma clipping</strong></a> is usually a good option to integrate more than 8 or 10 images. Keep in mind that for sigma clipping to work, the standard deviation must be a good estimator of dispersion, which requires a sufficient number of pixels per stack (the more images the better).</li>
<li class="pidoc_spaced_list_item"><a href="#winsorized_sigma_clipping"><strong>Winsorized sigma clipping</strong></a> is similar to the sigma clipping algorithm, but uses a special iterative procedure based on Huber's method of robust estimation of parameters through <em>Winsorization.</em> This algorithm can yield superior rejection of outliers with better preservation of significant data for large sets of images.</li>
<li class="pidoc_spaced_list_item"><a href="#averaged_sigma_clipping"><strong>Averaged sigma clipping</strong></a> is also a good algorithm for moderate sets from 8 to 10 images. This algorithm tries to derive the gain of an ideal CCD detector from existing pixel data, assuming zero readout noise, then uses a Poisson noise model to perform rejection. For larger sets of images however, sigma clipping tends to be superior.</li>
<li class="pidoc_spaced_list_item"><a href="#linear_fit_clipping"><strong>Linear fit clipping</strong></a> fits a straight line to the samples of each pixel stack using a robust line fitting algorithm. The linear fit is optimized in the twofold sense of minimizing average absolute deviation and maximizing inliers. This rejection algorithm is more robust than sigma clipping for large sets of images, especially in presence of additive sky gradients of varying intensity and spatial distribution. For the best performance, use this algorithm for large sets of at least 15 images; the more the better. Five images is the minimum required.</li>
<li class="pidoc_spaced_list_item"><a href="#ccd_noise_model_clipping"><strong>CCD noise model</strong></a> requires knowing accurate sensor parameters. This rejection algorithm can be mostly useful to integrate master calibration frames: bias, dark and flat master frames.</li>
</ul>

</dd>
<dt>
<p>Normalization</p>
</dt>
<dd>
<p>This is the <a href="#rejection_normalization">rejection normalization</a> method used exclusively for the pixel rejection task. Note that a different normalization is applied before image combination, which we call <a href="#output_normalization">output normalization</a>. Normalization is <em>essential</em> for a meaningful pixel rejection, since it ensures that the data from all the integrated images are statistically compatible in terms of mean background values and dispersion. Again, normalization methods have already been described in detail, so this is just a brief reference:</p>

<ul class="pidoc_list">
<li><strong>No normalization.</strong> This option <em>disables</em> rejection normalization. This is <em>only</em> recommended for integration of master bias and dark frames.</li>
<li class="pidoc_spaced_list_item"><a href="#scaling_plus_zero_offset"><strong>Scale + zero offset.</strong></a> Matches mean background values and dispersion. This involves multiplicative and additive transformations. This is the default rejection normalization method that should be used to integrate calibrated light frames.</li>
<li class="pidoc_spaced_list_item"><a href="#flux_equalization"><strong>Equalize fluxes.</strong></a> Matches the main histogram peaks of all images prior to pixel rejection. This is done by multiplication with the ratio of the reference median to the median of each integrated image. This is the method of choice to integrate sky flat fields, since in this case trying to match dispersion does not make sense, due to the irregular illumination distribution. For the same reason, this type of rejection normalization can also be useful to integrate uncalibrated images, or images suffering from <em>strong</em> gradients; however, in the latter case this method should only be selected as a last resort, when no pixel rejection algorithm is giving acceptable results.</li>
</ul>

</dd>
<dt>
<p>Generate rejection maps</p>
</dt>
<dd>
<p>This option must be enabled to generate <a href="#rejection_maps">rejection maps</a>, which are special control images to evaluate the performance of pixel rejection procedures. This option is enabled by default.</p>
</dd>
<dt>
<p>Clip low pixels</p>
</dt>
<dd>
<p>If this option is enabled, pixels below the reference value for each pixel stack (which is either the median or the fitted line in linear fit clipping rejection) will be rejected. Otherwise no pixels will be rejected below the reference values. This option is enabled by default. You can disable it to prevent rejecting any pixels on the background, although spurious dark image structures&mdash;such as dead or cold pixels for example&mdash;cannot be rejected if you do so.</p>
</dd>
<dt>
<p>Clip high pixels</p>
</dt>
<dd>
<p>If this option is enabled, pixels above the reference value for each pixel stack (which is either the median or the fitted line in linear fit clipping rejection) will be rejected. Otherwise no pixels will be rejected above the reference values. This option is enabled by default, and unless you have a really good reason to do otherwise, disabling it is not recommended.</p>
</dd>
<dt>
<p><a id="clip_low_range"></a> Clip low range</p>
</dt>
<dd>
<p>Enable this option to reject all pixels with values less than or equal to the <a href="#range_low">range low</a> parameter. This is very useful to reject black (or very dark) image areas at the edges of the images, caused by partial coverage due to image registration, or empty areas in mosaic frames.</p>
</dd>
<dt>
<p><a id="clip_high_range"></a> Clip high range</p>
</dt>
<dd>
<p>Enable this option to reject all pixels with values greater than or equal to the <a href="#range_high">range high</a> parameter. This is useful to reject white (or very bright) pixels.</p>
</dd>
</dl>

</div>

<div class="pidoc_subsection" id="__Usage_:_Pixel_Rejection_2__">
   <h4 class="pidoc_subsectionTitle">2.5&emsp;Pixel Rejection (2)</h4>
<img src="images/PixelRejection2.png" alt=""/>
<p>This section provides controls to define the <em>clipping points</em> for all pixel rejection algorithms except CCD noise model clipping:</p>

<dl class="pidoc_list">
<dt>
<p>Min/Max low, Min/Max high</p>
</dt>
<dd>
<p>The <img style="vertical-align:middle;" src="images/eqn_0055.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0056.svg" alt=""/> pixel counts, respectively, for the <a href="#minmax_clipping">min/max</a> rejection algorithm. From each pixel stack, the <img style="vertical-align:middle;" src="images/eqn_0055.svg" alt=""/> smallest and <img style="vertical-align:middle;" src="images/eqn_0056.svg" alt=""/> largest pixels will be rejected. The default value is one pixel for both parameters.</p>
</dd>
<dt>
<p>Percentile low, Percentile high</p>
</dt>
<dd>
<p>The <img style="vertical-align:middle;" src="images/eqn_0061.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0062.svg" alt=""/> parameters, respectively, of the <a href="#percentile_clipping">percentile clipping</a> rejection algorithm in units of the central value (median) of each pixel stack. The lower the value of one of these parameters, the more pixels will be rejected. The default values are 0.2 and 0.1, respectively.</p>
</dd>
<dt>
<p>Sigma low, Sigma high</p>
</dt>
<dd>
<p>The <img style="vertical-align:middle;" src="images/eqn_0067.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0068.svg" alt=""/> parameters, respectively, of the <a href="#sigma_clipping">sigma clipping</a>, <a href="#winsorized_sigma_clipping">Winsorized sigma clipping</a> and <a href="#averaged_sigma_clipping">averaged sigma clipping</a> rejection algorithms, in units of the dispersion (sigma) of each pixel stack. Efforts have been made to compatibilize the response of these three algorithms to both sigma parameters, but you may notice slight differences in performance for the same clipping points. The lower the value of one of these parameters, the more pixels will be rejected. The default values are 4 and 2, respectively.</p>
</dd>
<dt>
<p>Linear fit low, Linear fit high</p>
</dt>
<dd>
<p>The <img style="vertical-align:middle;" src="images/eqn_0067.svg" alt=""/> and <img style="vertical-align:middle;" src="images/eqn_0068.svg" alt=""/> parameters, respectively, of the <a href="#linear_fit_clipping">linear fit clipping</a> rejection algorithm, in units of the mean absolute deviation of the set of samples in each pixel stack from the fitted line, plus a 'magic factor' used internally to compatibilize the response of the linear fit clipping algorithm with sigma clipping. The lower the value of one of these parameters, the more pixels will be rejected. The default values are 5.0 and 2.5, respectively.</p>
</dd>
<dt>
<p><a id="range_low"></a> <a id="range_high"></a> Range low, Range high</p>
</dt>
<dd>
<p>The low and high <em>range rejection limits</em>. When the <a href="#clip_low_range">clip low range</a> and/or <a href="#clip_high_range">clip high range</a> options are enabled, pixels with values less than or equal to <em>range low</em> and/or greater than or equal to <em>range high</em> will be rejected. Note that from this definition, pure black and/or white pixels will always be rejected when the corresponding options are selected. Range rejection always takes place before the selected pixel rejection algorithm is applied. The default values are 0 and 0.98, respectively.</p>
</dd>
</dl>

</div>

<div class="pidoc_subsection" id="__Usage_:_Pixel_Rejection_3__">
   <h4 class="pidoc_subsectionTitle">2.6&emsp;Pixel Rejection (3)</h4>
<img src="images/PixelRejection3.png" alt=""/>
<p>This section provides controls to define the parameters of the <a href="#ccd_noise_model_clipping">CCD noise model clipping</a> pixel rejection algorithm:</p>

<dl class="pidoc_list">
<dt>
<p>CCD gain</p>
</dt>
<dd>
<p>CCD sensor gain in electrons per DN (data number), or e<sup>-</sup>/ADU. The default value is one electron per DN.</p>
</dd>
<dt>
<p>CCD readout noise</p>
</dt>
<dd>
<p>CCD readout noise in electrons. The default value is 10 e<sup>-</sup>.</p>
</dd>
<dt>
<p>CCD scale noise</p>
</dt>
<dd>
<p>Scale noise is also referred to as <em>sensitivity noise</em>. This is a dimensionless factor representing noise that is multiplicative in the integrated frames. Scale noise typically originates from flat fielding. This number is usually unknown, so the default value is zero.</p>
</dd>
</dl>

</div>

<div class="pidoc_subsection" id="__Usage_:_Region_of_Interest__">
   <h4 class="pidoc_subsectionTitle">2.7&emsp;Region of Interest</h4>
<img src="images/RegionOfInterest.png" alt=""/>
<p>To optimize an image integration process, usually a large number of tests are necessary to fine tune the many intervening parameters, especially pixel rejection parameters. To speed up the process, a rectangular <em>region of interest</em> (ROI) can be defined to restrict the process to the pixels pertaining to the specified area.</p>

<dl class="pidoc_list">
<dt>
<p>Left, Top, Width, Height</p>
</dt>
<dd>
<p>Define the position of the upper left corner and the dimensions of the rectangular ROI, in image pixel units.</p>
</dd>
<dt>
<p>From Preview</p>
</dt>
<dd>
<p>Click this button to open a dialog where an existing preview can be selected to copy its geometry to the ROI. In general, you'll want to implement the following sequence:</p>

<ul class="pidoc_list">
<li>Activate (double-click, or select and press Enter) one of the images in the <a href="#input_images_list">input list</a> to open it.</li>
<li>Apply an automatic screen stretch with the <a href="../../tools/ScreenTransferFunction/ScreenTransferFunction.html" title="../../tools/ScreenTransferFunction/ScreenTransferFunction.html">ScreenTransferFunction</a> tool. This step is necessary because the images to be integrated are (must be!) linear.</li>
<li>Define a preview covering a relatively small area of special interest. For example, you may want to include some bright nebular objects, stars, and some sky background.</li>
<li>Click the <em>From Preview</em> button and select the preview to copy its geometry to the ROI.</li>
</ul>

</dd>
</dl>

</div>

<div class="pidoc_subsection" id="__Usage_:_Rejection_Maps__">
   <h4 class="pidoc_subsectionTitle">2.8&emsp;Rejection Maps</h4>
<p>A rejection map is a special control image that provides information about the amount and location of rejected pixels. Each pixel in a rejection map has a real value in the [0,1] range, the value being proportional to the number of rejected pixels: if a rejection map pixel is zero, then no pixel has been rejected at the corresponding coordinates; contrarily, if a rejection map pixel has a value of one, it indicates that <em>all</em> pixels have been rejected at the corresponding location. Rejection maps are useful data structures to evaluate the performance of a pixel rejection task. The ImageIntegration tool provides two rejection maps for each pixel rejection algorithm: <em>low</em> and <em>high</em> rejection maps respectively for low and high rejected pixels.</p>
<p>In addition, the <a href="#linear_fit_clipping">linear fit clipping</a> algorithm generates <em>slope maps</em>, whose values are proportional to the slopes of the fitted lines for the integrated pixel stacks. In a slope map image, each pixel indicates the slope angle in the range from black=0&deg; to white=90&deg;. A slope map informs you about the magnitude and spatial distribution of brightness variations in your data set.</p>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>

<div class="pidoc_figure">
<a id="__figure_10__"></a>
<p><span class="pidoc_figure_title">Figure 10 &mdash;</span>  <strong>Rejection of Plane Trails</strong></p>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>
<div class="pidoc_mouseover">
<div class="pidoc_image_right"><img src="images/RejectionExampleHighMap.png" id="xVGS6SKBYe6pom5R" alt="" /></div>
<ul>
<li><span class="pidoc_indicator_default" id="xVGS6SKBYe6pom5R_1"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('xVGS6SKBYe6pom5R', 'images/RejectionExampleHighMap.png'); pidoc_hideGroup('xVGS6SKBYe6pom5R', 5); pidoc_setOpacity('xVGS6SKBYe6pom5R_1', 1.0);">High rejection map</a></li>
<li><span class="pidoc_indicator" id="xVGS6SKBYe6pom5R_2"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('xVGS6SKBYe6pom5R', 'images/RejectionExampleLowMap.png'); pidoc_hideGroup('xVGS6SKBYe6pom5R', 5); pidoc_setOpacity('xVGS6SKBYe6pom5R_2', 1.0);">Low rejection map</a></li>
<li><span class="pidoc_indicator" id="xVGS6SKBYe6pom5R_3"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('xVGS6SKBYe6pom5R', 'images/RejectionExampleIntegrated.png'); pidoc_hideGroup('xVGS6SKBYe6pom5R', 5); pidoc_setOpacity('xVGS6SKBYe6pom5R_3', 1.0);">Integrated image</a></li>
<li><span class="pidoc_indicator" id="xVGS6SKBYe6pom5R_4"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('xVGS6SKBYe6pom5R', 'images/RejectionExampleMask.png'); pidoc_hideGroup('xVGS6SKBYe6pom5R', 5); pidoc_setOpacity('xVGS6SKBYe6pom5R_4', 1.0);">Integrated, masked w/ high map</a></li>
<li><span class="pidoc_indicator" id="xVGS6SKBYe6pom5R_5"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('xVGS6SKBYe6pom5R', 'images/RejectionExampleNoRejection.png'); pidoc_hideGroup('xVGS6SKBYe6pom5R', 5); pidoc_setOpacity('xVGS6SKBYe6pom5R_5', 1.0);">Integrated image, no rejection</a></li>
</ul>
</div>

<div class="pidoc_vspacer" style="margin-top:0.5em;"></div>
<p>Ten H&alpha; images have been integrated with average combination and the Winsorized sigma clipping rejection algorithm. Rejection clipping points have been set to 4.3 and 3.2 sigma, respectively for low and high pixels. In the comparison above, you can see the rejection maps, the integrated image, the integrated image with the high rejection map selected as an inverted mask, and the result of the same integration without pixel rejection. The big rejected blob near the top left corner corresponds to a plane lights flash. Images courtesy of Oriol Lehmkuhl and Ivette Rodr√≠guez.</p>
</div>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>

<div class="pidoc_figure">
<a id="__figure_11__"></a>
<p><span class="pidoc_figure_title">Figure 11 &mdash;</span>  <strong>Slope Maps</strong></p>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>
<div class="pidoc_mouseover">
<div class="pidoc_image_right"><img src="images/SlopeExampleImage1.png" id="12myY0ygG7xP1ZL5" alt="" /></div>
<ul>
<li><span class="pidoc_indicator_default" id="12myY0ygG7xP1ZL5_1"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('12myY0ygG7xP1ZL5', 'images/SlopeExampleImage1.png'); pidoc_hideGroup('12myY0ygG7xP1ZL5', 3); pidoc_setOpacity('12myY0ygG7xP1ZL5_1', 1.0);">First image of series, gradient at bottom</a></li>
<li><span class="pidoc_indicator" id="12myY0ygG7xP1ZL5_2"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('12myY0ygG7xP1ZL5', 'images/SlopeExampleImage2.png'); pidoc_hideGroup('12myY0ygG7xP1ZL5', 3); pidoc_setOpacity('12myY0ygG7xP1ZL5_2', 1.0);">Last image of series, gradient at top</a></li>
<li><span class="pidoc_indicator" id="12myY0ygG7xP1ZL5_3"></span><a href="javascript:void(0);" onmouseover="pidoc_setImgSrc('12myY0ygG7xP1ZL5', 'images/SlopeExampleMap.png'); pidoc_hideGroup('12myY0ygG7xP1ZL5', 3); pidoc_setOpacity('12myY0ygG7xP1ZL5_3', 1.0);">Slope map</a></li>
</ul>
</div>

<div class="pidoc_vspacer" style="margin-top:0.5em;"></div>
<p>Twenty luminance images of the M81/M82 region have been integrated with average combination and the Linear fit clipping rejection algorithm. In this series of images, light-pollution sky gradients have reversed orientations, dividing the set of images into two well differentiated groups: one where the gradients grow toward the bottom of the image, and a second group with gradients in the opposite direction. These variations are caused by a change of orientation of the telescope. In the comparison above you can see how the slope map reproduces the combined gradients as the slopes of the fitted lines for all integrated pixels. Images courtesy of Oriol Lehmkuhl and Ivette Rodr√≠guez.</p>
</div>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>

<div class="pidoc_figure">
<a id="__figure_12__"></a>
<p><span class="pidoc_figure_title">Figure 12 &mdash;</span>  <strong>Slope Map Variations</strong></p>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>
<img style="border:1px solid black;" src="images/SlopeMapDetail.png" alt=""/>

<div class="pidoc_vspacer" style="margin-top:1em;"></div>
<p>Slope maps are brighter for high SNR regions, denoting larger fluctuations on these areas throughout the set of integrated images. These fluctuations are a direct consequence of CCD pixel statistics: the expected value of a Poisson distribution is equal to its variance, hence the variations are stronger on brighter areas.</p>
<p>A good example is shown on this crop of the slope map generated for the M81/M82 integration performed in the previous figure. Interestingly, there seem to be delimiting regions around bright objects where these fluctuations decrease significantly for some reason that eludes us. This effect can be seen as dark rings around the stars projected over M81 in the image above. Note that despite the 'impressive' appearance of the stretched slope map image that we are representing here, actual pixel values range from 0.02 in the core of M81 to about 0.0001 on sky background areas. These values correspond to line slopes of 1.15 degrees and 20 arcseconds, respectively.</p>
</div>
</div>

<div class="pidoc_subsection" id="__Usage_:_Console_Statistics__">
   <h4 class="pidoc_subsectionTitle">2.9&emsp;Console Statistics</h4>
<p>As is customary in PixInsight, the ImageIntegration tool provides extensive information about the ongoing processes and the achieved results on the processing console. The information generated is both quantitative and qualitative. You should understand and know how to evaluate this information in order to achieve the best possible result out of your data set.</p>
<div class="pidoc_subsection" id="__Usage_:_Console_Statistics_:_Information_About_Input_Images__">
   <h5 class="pidoc_subsectionTitle">2.9.1&emsp;Information About Input Images</h5>
<p>The first step in the ImageIntegration process is computing several statistical properties for each input image. If a valid <a href="#cache_management">cache</a> entry exists for an input file, statistical data are retrieved from the cache in negligible time. If no valid cache entry exists (e.g., because the file in question has never been integrated, because the required cache items are not available, because the cache has expired, or if the file's time stamp is newer than the version in the cache), then the whole image is loaded and the necessary data are computed. For large images and image sets, this process may take some time. Irrespective of the source of statistical data, the following information is provided for each input file:</p>

<ul class="pidoc_list">
<li>Full file path in the local filesystem.</li>
<li>Sample data format and geometry, provided by the underlying file format support module.</li>
<li>Scaling factors, relative to the reference image.</li>
<li>Zero offsets, relative to the reference image.</li>
<li>MRS noise estimates in sigma units.</li>
<li>Image weighting factors.</li>
</ul>

<p>The four last items are provided for each nominal channel. Scaling factors and image weights are relative to the reference image, that is, to the first input image. In the example below you can see a fragment of the information given for a typical image integration task.</p>

<div class="pidoc_box">

<pre>Opening files:
/home/juan/tmp/18/registered/Blue/M81-M82-B-001-crop2K_c_r.fit
* Retrieved data from file cache.
Scale factors   :   1.00000
Zero offset     :  +0.000000e+00
Noise estimates :  2.4060e-04
Weight          :     1.00000
/home/juan/tmp/18/registered/Blue/M81-M82-B-002-crop2K_c_r.fit
* Retrieved data from file cache.
Scale factors   :   0.98579
Zero offset     :  -3.890576e-05
Noise estimates :  2.4150e-04
Weight          :     1.02138
/home/juan/tmp/18/registered/Blue/M81-M82-B-003-crop2K_c_r.fit
* Retrieved data from file cache.
Scale factors   :   0.98003
Zero offset     :  -1.989279e-04
Noise estimates :  2.4500e-04
Weight          :     1.00411</pre>

</div>
</div>

<div class="pidoc_subsection" id="__Usage_:_Console_Statistics_:_Information_About_the_Integration_Process__">
   <h5 class="pidoc_subsectionTitle">2.9.2&emsp;Information About the Integration Process</h5>
<p>Just before starting to integrate pixels, ImageIntegration writes a summary of parameters to the console. This can be useful to keep track of several executions of the tool for comparison of results. The box below shows a typical example.</p>

<div class="pidoc_box">

<pre>Integration of 15 images:
Pixel combination ......... average
Output normalization ...... additive + scaling
Weighting mode ............ noise evaluation
Scale estimator ........... iterative k-sigma / BWMV
Range rejection ........... range_low=0.000000 range_high=0.980000
Pixel rejection ........... Winsorized sigma clipping
Rejection normalization ... scale + zero offset
Rejection clippings ....... low=yes high=yes
Rejection parameters ...... sigma_low=6.000 sigma_high=4.200

* Using 2048 concurrent pixel stack(s) = 736.00 MB
Integrating pixel rows:     0 -&gt;  2047: done</pre>

</div>
</div>

<div class="pidoc_subsection" id="__Usage_:_Console_Statistics_:_Pixel_Rejection_Counts__">
   <h5 class="pidoc_subsectionTitle">2.9.3&emsp;Pixel Rejection Counts</h5>
<p>At the end of the integration process, a complete summary of pixel rejection results is written to the console. An example is shown in the box below.</p>

<div class="pidoc_box">

<pre>Pixel rejection counts:
/home/juan/tmp/18/registered/Blue/M81-M82-B-001-crop2K_c_r.fit
    1 :     88524   2.111% (    81726 +      6798 =   1.948% +   0.162%)
/home/juan/tmp/18/registered/Blue/M81-M82-B-002-crop2K_c_r.fit
    2 :    116132   2.769% (   109492 +      6640 =   2.610% +   0.158%)
/home/juan/tmp/18/registered/Blue/M81-M82-B-003-crop2K_c_r.fit
    3 :     90520   2.158% (    83554 +      6966 =   1.992% +   0.166%)
/home/juan/tmp/18/registered/Blue/M81-M82-B-004-crop2K_c_r.fit
    4 :    114049   2.719% (   108217 +      5832 =   2.580% +   0.139%)
/home/juan/tmp/18/registered/Blue/M81-M82-B-005-crop2K_c_r.fit
    5 :     49877   1.189% (    45129 +      4748 =   1.076% +   0.113%)
/home/juan/tmp/18/registered/Blue/M81-M82-B-006-crop2K_c_r.fit
    6 :     74476   1.776% (    69623 +      4853 =   1.660% +   0.116%)
/home/juan/tmp/18/registered/Blue/M81-M82-B-007-crop2K_c_r.fit
    7 :     44600   1.063% (    40509 +      4091 =   0.966% +   0.098%)
/home/juan/tmp/18/registered/Blue/M81-M82-B-008-crop2K_c_r.fit
    8 :     46470   1.108% (    42869 +      3601 =   1.022% +   0.086%)
/home/juan/tmp/18/registered/Blue/M81-M82-B-009-crop2K_c_r.fit
    9 :     24781   0.591% (    22196 +      2585 =   0.529% +   0.062%)
/home/juan/tmp/18/registered/Blue/M81-M82-B-010-crop2K_c_r.fit
   10 :     38032   0.907% (    35522 +      2510 =   0.847% +   0.060%)
/home/juan/tmp/18/registered/Blue/M81-M82-B-011-crop2K_c_r.fit
   11 :     27588   0.658% (    25141 +      2447 =   0.599% +   0.058%)
/home/juan/tmp/18/registered/Blue/M81-M82-B-012-crop2K_c_r.fit
   12 :     37152   0.886% (    34911 +      2241 =   0.832% +   0.053%)
/home/juan/tmp/18/registered/Blue/M81-M82-B-013-crop2K_c_r.fit
   13 :     60279   1.437% (    57837 +      2442 =   1.379% +   0.058%)
/home/juan/tmp/18/registered/Blue/M81-M82-B-014-crop2K_c_r.fit
   14 :     46786   1.115% (    44337 +      2449 =   1.057% +   0.058%)
/home/juan/tmp/18/registered/Blue/M81-M82-B-015-crop2K_c_r.fit
   15 :    133531   3.184% (   129814 +      3717 =   3.095% +   0.089%)

Total :    992797   1.578% (   930877 +     61920 =   1.480% +   0.098%)</pre>

</div>
<p>For each input image, the information given has the following format:</p>

<pre> &lt;index&gt; : &lt;total-count&gt; &lt;total-pc&gt;% (&lt;low-count&gt; + &lt;high-count&gt; = &lt;low-pc&gt;% + &lt;high-pc&gt;%)</pre>

<p>where:</p>

<ul class="pidoc_list">
<li><span class="pidoc_code">&lt;index&gt;</span> is the position of the image in the input list, from one to the number of images.</li>
<li><span class="pidoc_code">&lt;total-count&gt;</span> is the total number of rejected pixels.</li>
<li><span class="pidoc_code">&lt;total-pc&gt;</span> is the percentage of rejected pixels, with respect to the number of pixels in the image (width&times;height).</li>
<li><span class="pidoc_code">&lt;low-count&gt;</span> is the number of rejected pixels below their stack's central value, or <em>rejected low pixels</em>.</li>
<li><span class="pidoc_code">&lt;low-pc&gt;</span> is the percentage of rejected low pixels, with respect to the number of pixels in the image.</li>
<li><span class="pidoc_code">&lt;high-count&gt;</span> is the number of rejected pixels above their stack's central value, or <em>rejected high pixels</em>.</li>
<li><span class="pidoc_code">&lt;high-pc&gt;</span> is the percentage of rejected high pixels, with respect to the number of pixels in the image.</li>
</ul>

<p>Finally, the same information is given for the whole process, where each item is the sum of the same data items for all the integrated images.</p>
</div>

<div class="pidoc_subsection" id="__Usage_:_Console_Statistics_:_Noise_Evaluation_Statistics__">
   <h5 class="pidoc_subsectionTitle">2.9.4&emsp;Noise Evaluation Statistics</h5>
<p>When the <a href="#evaluate_noise">Evaluate noise</a> option is selected, ImageIntegration performs a final <a href="#quality_assessment">quality assessment</a> step. Noise, scale and location estimates are computed for each nominal channel of the output integrated image with the <a href="#noise_evaluation_weighting">MRS noise evaluation</a> and <a href="#ikss_estimator">IKSS</a> algorithms. From these estimates, robust effective noise reduction function values are calculated (see Equation <a href="#__equation_40__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 40]<br/>
<img src='images/eqn_0140.svg' alt=''/>">[40]</a>). The following box shows an example.</p>

<div class="pidoc_box">

<pre>MRS noise evaluation: done
SNR evaluation: done
Computing noise scaling factors: done

Gaussian noise estimates  : 5.5914e-05
Scale estimates           : 7.5562e-05
Location estimates        : 3.9906e-03
SNR estimates             : 2.1721e+04
Reference noise reduction : 1.4261
Median noise reduction    : 1.4214</pre>

</div>
<p>The <em>reference noise reduction</em> value is relative to the reference image (the first input image), while the <em>median noise reduction</em> is the median of computed values for all images; this is the most significant value that should be maximized to achieve the best possible SNR increment. Finally, an <em>approximate</em> SNR value is computed using Equation <a href="#__equation_12__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Equation 12]<br/>
<img src='images/eqn_0048.svg' alt=''/>">[12]</a>.</p>
</div>

</div>

<div class="pidoc_subsection" id="__Usage_:_Cache_Management__">
   <h4 class="pidoc_subsectionTitle">2.10&emsp;Cache Management</h4>
<p>The ImageIntegration tool uses a persistent file cache to store statistical properties of images. This includes noise estimates and estimates of location and scale such as the mean, median, MAD, IKSS, etc., among many other values, as required by the different integration processes that are executed. Storage of these precalculated properties greatly improves efficiency of the image integration process when repeated executions are necessary to fine tune pixel rejection and image normalization parameters. If a file has already been involved in an integration process, instead of recomputing the necessary data they are retrieved from the cache in insignificant time. In a similar way to development utilities such as <a href="http://www.gnu.org/s/make/" title="http://www.gnu.org/s/make/">make,</a> ImageIntegration validates stored cache items by comparing file modification times. If a file has been modified since the corresponding data was stored in the cache, the data are recalculated and stored in the cache again.</p>
<p>The cache can be managed from ImageIntegration's preferences. To access these settings, click the Preferences button on ImageIntegration's control bar, as shown here.</p>

<div style="text-align:center;">
<img src="images/PreferencesButton.png" alt=""/>
</div>
<p>This will open the Cache Preferences dialog, which provides the parameters described below.</p>

<div style="text-align:center;">
<img src="images/CachePreferencesDialog.png" alt=""/>
</div>

<dl class="pidoc_list">
<dt>
<p>Persistent file cache</p>
</dt>
<dd>
<p>Enable this option to use a persistent file cache to store statistical data and noise estimates for all integrated images. A persistent cache is kept across PixInsight sessions. If you disable this option, the file cache will still be used, but only during the current session: as soon as you exit the PixInsight Core application, all the cached information will be lost. With the persistent cache option enabled, all cache items will be stored and will be available the next time you run PixInsight. This option is enabled by default.</p>
</dd>
<dt>
<p>Cache duration</p>
</dt>
<dd>
<p>Persistent file cache items can be automatically removed after a specified period without accessing the corresponding files. Enter the desired period in days, or specify zero to disable this <em>automatic purge</em> feature, so that existing file cache items will never expire. The default cache duration is 30 days.</p>
</dd>
<dt>
<p>Clear Memory Cache Now</p>
</dt>
<dd>
<p>Click this button to remove all cache items currently stored in volatile RAM.</p>
</dd>
<dt>
<p>Purge Persistent Cache Now</p>
</dt>
<dd>
<p>Click this button to remove all stored persistent cache items. <strong>Warning: This action cannot be undone.</strong></p>
</dd>
</dl>

</div>

<div class="pidoc_subsection" id="__Usage_:_Scripting_and_Automation__">
   <h4 class="pidoc_subsectionTitle">2.11&emsp;Scripting and Automation</h4>
<p>The ImageIntegration process can be easily automated via scripting with the PixInsight JavaScript Runtime (PJSR). This allows you to use ImageIntegration in the context of more complex systems, such as preprocessing pipelines.</p>
<p>To automate ImageIntegration execution, a script must invoke the <span class="pidoc_code">executeGlobal</span> method of an ImageIntegration instance. By setting appropriate process parameters before this call, the script can specify a list of input images and all the required operating parameters. After successful execution, the ImageIntegration instance will provide a number of read-only properties with complete information about the integration process performed. The following tables describe all ImageIntegration parameters and properties available for scripting:</p>

<table class="pidoc_table" style="width:100%;">
<caption><a id="__table_5__"></a>
<span class="pidoc_table_title">Table 5 &mdash;  ImageIntegration Parameters</span></caption>
<tr>
<th><p>Parameter</p>
</th>
<th><p>Data Type</p>
</th>
<th><p>Description</p>
</th>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">bufferSizeMB</span></strong></span></td>
<td><p>Int32</p>
</td>
<td><p>Integration buffer size in mebibytes (MiB).</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">ccdGain</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>CCD gain in electrons per DN.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">ccdReadNoise</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>CCD readout noise in electrons.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">ccdScaleNoise</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>CCD scale noise (dimensionless)</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">clipHigh</span></strong></span></td>
<td><p>Boolean</p>
</td>
<td><p>Enable high pixel clipping.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">clipLow</span></strong></span></td>
<td><p>Boolean</p>
</td>
<td><p>Enable low pixel clipping.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">closePreviousImages</span></strong></span></td>
<td><p>Boolean</p>
</td>
<td><p>Close image windows generated by a previous instance.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">combination</span></strong></span></td>
<td><p>Int32</p>
</td>
<td><p>Image combination operation. One of:</p>
<p>ImageIntegration.prototype.Average<br/>
 ImageIntegration.prototype.Median<br/>
 ImageIntegration.prototype.Minimum<br/>
 ImageIntegration.prototype.Maximum</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">evaluateNoise</span></strong></span></td>
<td><p>Boolean</p>
</td>
<td><p>Perform a final quality assessment step.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">generate64BitResult</span></strong></span></td>
<td><p>Boolean</p>
</td>
<td><p>Generate the integrated result as a 64-bit floating point image (32-bit floating point otherwise).</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">generateIntegratedImage</span></strong></span></td>
<td><p>Boolean</p>
</td>
<td><p>Enable generation of the output integrated image.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">generateRejectionMaps</span></strong></span></td>
<td><p>Boolean</p>
</td>
<td><p>Enable generation of rejection map images.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">ignoreNoiseKeywords</span></strong></span></td>
<td><p>Boolean</p>
</td>
<td><p>Do not retrieve noise estimates from NOISExx FITS header keywords; either recalculate noise estimates or read them from existing cache items.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">images</span></strong></span></td>
<td><p>Table</p>
</td>
<td><p>Input images. This table has two column parameters:</p>

<table class="pidoc_table" style="margin-top:1em;width:100%;">
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">enabled</span></strong></span></td>
<td><p>Boolean</p>
</td>
<td><p>Enabled state. Disabled input files are not integrated.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">path</span></strong></span></td>
<td><p>String</p>
</td>
<td><p>Full path of this input file.</p>
</td>
</tr>
</table>

</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">inputHints</span></strong></span></td>
<td><p>String</p>
</td>
<td><p>Input hints string. Multiple hints as well as hint parameters must be separated by spaces.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">linearFitHigh</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>High clipping point (in average absolute deviation units) for linear fit clipping rejection.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">linearFitLow</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Low clipping point (in average absolute deviation units) for linear fit clipping rejection.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">minMaxHigh</span></strong></span></td>
<td><p>Int32</p>
</td>
<td><p>Number of clipped high pixels, min/max rejection algorithm.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">minMaxLow</span></strong></span></td>
<td><p>Int32</p>
</td>
<td><p>Number of clipped low pixels, min/max rejection algorithm.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">mrsMinDataFraction</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Minimum fraction of pixels in a valid MRS noise estimate.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">noGUIMessages</span></strong></span></td>
<td><p>Boolean</p>
</td>
<td><p>Do not use GUI resources (such as message boxes) to provide information to the user; use console output exclusively.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">normalization</span></strong></span></td>
<td><p>Enumeration</p>
</td>
<td><p>Output image normalization method. One of:</p>
<p>ImageIntegration.prototype.DontCare<br/>
 ImageIntegration.prototype.ExposureTime<br/>
 ImageIntegration.prototype.NoiseEvaluation<br/>
 ImageIntegration.prototype.SignalWeight<br/>
 ImageIntegration.prototype.MedianWeight<br/>
 ImageIntegration.prototype.AverageWeight<br/>
 ImageIntegration.prototype.KeywordWeight</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">pcClipHigh</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>High clipping point (relative to the median) for the percentile clipping rejection algorithm.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">pcClipLow</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Low clipping point (relative to the median) for the percentile clipping rejection algorithm.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">rangeClipHigh</span></strong></span></td>
<td><p>Boolean</p>
</td>
<td><p>Enable high range rejection.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">rangeClipLow</span></strong></span></td>
<td><p>Boolean</p>
</td>
<td><p>Enable low range rejection.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">rangeHigh</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>High range rejection point in the [0,1] range.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">rangeLow</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Low range rejection point in the [0,1] range.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">rejection</span></strong></span></td>
<td><p>Enumeration</p>
</td>
<td><p>Pixel rejection algorithm. One of:</p>
<p>ImageIntegration.prototype.NoRejection<br/>
 ImageIntegration.prototype.MinMax<br/>
 ImageIntegration.prototype.PercentileClip<br/>
 ImageIntegration.prototype.SigmaClip<br/>
 ImageIntegration.prototype.WinsorizedSigmaClip<br/>
 ImageIntegration.prototype.AveragedSigmaClip<br/>
 ImageIntegration.prototype.LinearFit<br/>
 ImageIntegration.prototype.CCDClip</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">rejectionNormalization</span></strong></span></td>
<td><p>Enumeration</p>
</td>
<td><p>Image normalization method (rejection normalization). One of:</p>
<p>ImageIntegration.prototype.NoRejectionNormalization<br/>
 ImageIntegration.prototype.Scale<br/>
 ImageIntegration.prototype.EqualizeFluxes</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">roiX0</span></strong></span></td>
<td><p>Int32</p>
</td>
<td><p>ROI rectangle, left pixel coordinate.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">roiX1</span></strong></span></td>
<td><p>Int32</p>
</td>
<td><p>ROI rectangle, right pixel coordinate (excluded, ROI width = roiX1 - roiX0).</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">roiY0</span></strong></span></td>
<td><p>Int32</p>
</td>
<td><p>ROI rectangle, top pixel coordinate.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">roiY1</span></strong></span></td>
<td><p>Int32</p>
</td>
<td><p>ROI rectangle, bottom pixel coordinate (excluded, ROI height = roiY1 - roiY0).</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">sigmaHigh</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>High clipping point (in sigma units) for the sigma clipping, averaged sigma clipping and Winsorized sigma clipping rejection algorithms.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">sigmaLow</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Low clipping point (in sigma units) for the sigma clipping, averaged sigma clipping and Winsorized sigma clipping rejection algorithms.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">stackSizeMB</span></strong></span></td>
<td><p>Int32</p>
</td>
<td><p>Integration stack size in mebibytes (MiB).</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">useCache</span></strong></span></td>
<td><p>Boolean</p>
</td>
<td><p>Enable the persistent file cache.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">useROI</span></strong></span></td>
<td><p>Boolean</p>
</td>
<td><p>Enable the region of interest (ROI).</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">weightKeyword</span></strong></span></td>
<td><p>String</p>
</td>
<td><p>The name of a FITS header keyword to retrieve image weights, only if normalization = KeywordWeight.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">weightMode</span></strong></span></td>
<td><p>Enumeration</p>
</td>
<td><p>Image weighting method. One of:</p>
<p>ImageIntegration.prototype.NoNormalization<br/>
 ImageIntegration.prototype.Additive<br/>
 ImageIntegration.prototype.Multiplicative<br/>
 ImageIntegration.prototype.AdditiveWithScaling<br/>
 ImageIntegration.prototype.MultiplicativeWithScaling</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">weightScale</span></strong></span></td>
<td><p>Enumeration</p>
</td>
<td><p>Statistical estimator of scale. One of:</p>
<p>ImageIntegration.WeightScale_AvgDev<br/>
 ImageIntegration.WeightScale_BWMV<br/>
 ImageIntegration.WeightScale_IKSS<br/>
 ImageIntegration.WeightScale_MAD<br/>
 ImageIntegration.WeightScale_PBMV<br/>
 ImageIntegration.WeightScale_Qn<br/>
 ImageIntegration.WeightScale_Sn</p>
</td>
</tr>
</table>


<table class="pidoc_table" style="width:100%;">
<caption><a id="__table_6__"></a>
<span class="pidoc_table_title">Table 6 &mdash;  ImageIntegration Output Properties</span></caption>
<tr>
<th><p>Property</p>
</th>
<th><p>Data Type</p>
</th>
<th><p>Description</p>
</th>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">finalLocationEstimateB</span></strong></span></td>
<td><p>Double</p>
</td>
<td><p>IKSS location estimate for the output integrated image, blue channel. Zero if not calculated</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">finalLocationEstimateG</span></strong></span></td>
<td><p>Double</p>
</td>
<td><p>IKSS location estimate for the output integrated image, green channel. Zero if not calculated</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">finalLocationEstimateRK</span></strong></span></td>
<td><p>Double</p>
</td>
<td><p>IKSS location estimate for the output integrated image, red or gray channel. Zero if not calculated</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">finalNoiseEstimateB</span></strong></span></td>
<td><p>Double</p>
</td>
<td><p>MRS Gaussian noise estimate for the output integrated image, blue channel. Zero if not calculated</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">finalNoiseEstimateG</span></strong></span></td>
<td><p>Double</p>
</td>
<td><p>MRS Gaussian noise estimate for the output integrated image, green channel. Zero if not calculated</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">finalNoiseEstimateRK</span></strong></span></td>
<td><p>Double</p>
</td>
<td><p>MRS Gaussian noise estimate for the output integrated image, red or gray channel. Zero if not calculated</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">finalScaleEstimateB</span></strong></span></td>
<td><p>Double</p>
</td>
<td><p>IKSS scale estimate for the output integrated image, blue channel. Zero if not calculated</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">finalScaleEstimateG</span></strong></span></td>
<td><p>Double</p>
</td>
<td><p>IKSS scale estimate for the output integrated image, green channel. Zero if not calculated</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">finalScaleEstimateRK</span></strong></span></td>
<td><p>Double</p>
</td>
<td><p>IKSS scale estimate for the output integrated image, red or gray channel. Zero if not calculated</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">highRejectionMapImageId</span></strong></span></td>
<td><p>String</p>
</td>
<td><p>Identifier of the high rejection map image, or an empty string if no high rejection map has been generated.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">imageData</span></strong></span></td>
<td><p>Table</p>
</td>
<td><p>Data about the integrated images. Each row in this table provides information on the corresponding image in the input list. Disabled input images are not included in this table.</p>

<table class="pidoc_table" style="margin-top:1em;width:100%;">
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">weightB</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Image weight relative to the reference image, blue channel.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">weightG</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Image weight relative to the reference image, green channel.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">weightRK</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Image weight relative to the reference image, red or gray channel.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">rejectedLowB</span></strong></span></td>
<td><p>UInt64</p>
</td>
<td><p>Number of rejected low pixels, blue channel.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">rejectedLowG</span></strong></span></td>
<td><p>UInt64</p>
</td>
<td><p>Number of rejected low pixels, green channel.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">rejectedLowRK</span></strong></span></td>
<td><p>UInt64</p>
</td>
<td><p>Number of rejected low pixels, red or gray channel.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">rejectedHighB</span></strong></span></td>
<td><p>UInt64</p>
</td>
<td><p>Number of rejected high pixels, blue channel.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">rejectedHighG</span></strong></span></td>
<td><p>UInt64</p>
</td>
<td><p>Number of rejected high pixels, green channel.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">rejectedHighRK</span></strong></span></td>
<td><p>UInt64</p>
</td>
<td><p>Number of rejected high pixels, red or gray channel.</p>
</td>
</tr>
</table>

</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">integrationImageId</span></strong></span></td>
<td><p>String</p>
</td>
<td><p>Identifier of the output integrated image, or an empty string if no integrated image has been generated.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">lowRejectionMapImageId</span></strong></span></td>
<td><p>String</p>
</td>
<td><p>Identifier of the low rejection map image, or an empty string if no low rejection map has been generated.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">medianNoiseReductionB</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Median effective noise reduction, blue channel. Zero if not calculated.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">medianNoiseReductionG</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Median effective noise reduction, green channel. Zero if not calculated.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">medianNoiseReductionRK</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Median effective noise reduction, red or gray channel. Zero if not calculated.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">numberOfChannels</span></strong></span></td>
<td><p>Int32</p>
</td>
<td><p>Number of channels in the integrated image.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">numberOfPixels</span></strong></span></td>
<td><p>UInt64</p>
</td>
<td><p>Area of the integrated image in square pixels.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">referenceNoiseReductionB</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Effective noise reduction with respect to the reference image, blue channel. Zero if not calculated.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">referenceNoiseReductionG</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Effective noise reduction with respect to the reference image, green channel. Zero if not calculated.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">referenceNoiseReductionRK</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Effective noise reduction with respect to the reference image, red or gray channel. Zero if not calculated.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">referenceSNRIncrementB</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Deprecated - should be ignored.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">referenceSNRIncrementG</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Deprecated - should be ignored.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">referenceSNRIncrementRK</span></strong></span></td>
<td><p>Float</p>
</td>
<td><p>Deprecated - should be ignored.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">slopeMapImageId</span></strong></span></td>
<td><p>String</p>
</td>
<td><p>Identifier of the slope map image, or an empty string if no slope map has been generated.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">totalPixels</span></strong></span></td>
<td><p>UInt64</p>
</td>
<td><p>Total integrated pixels, or the volume of the total pixel stack in cubic pixels: numberOfPixels&times;numberOfFiles.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">totalRejectedHighB</span></strong></span></td>
<td><p>UInt64</p>
</td>
<td><p>Number of rejected high pixels, blue channel.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">totalRejectedHighG</span></strong></span></td>
<td><p>UInt64</p>
</td>
<td><p>Number of rejected high pixels, green channel.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">totalRejectedHighRK</span></strong></span></td>
<td><p>UInt64</p>
</td>
<td><p>Number of rejected high pixels, red or gray channel.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">totalRejectedLowB</span></strong></span></td>
<td><p>UInt64</p>
</td>
<td><p>Number of rejected low pixels, blue channel.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">totalRejectedLowG</span></strong></span></td>
<td><p>UInt64</p>
</td>
<td><p>Number of rejected low pixels, green channel.</p>
</td>
</tr>
<tr>
<td><span class="pidoc_nowrap"><strong><span class="pidoc_code">totalRejectedLowRK</span></strong></span></td>
<td><p>UInt64</p>
</td>
<td><p>Number of rejected low pixels, red or gray channel.</p>
</td>
</tr>
</table>

</div>

<div class="pidoc_subsection" id="__Usage_:_Usage_Hints__">
   <h4 class="pidoc_subsectionTitle">2.12&emsp;Usage Hints</h4>

<ul class="pidoc_list">
<li>Use the <a href="../../scripts/SubframeSelector/SubframeSelector.html" title="../../scripts/SubframeSelector/SubframeSelector.html">SubframeSelector</a> script and the <a href="../../tools/Blink/Blink.html" title="../../tools/Blink/Blink.html">Blink</a> tool to analyze your data and grade your images, both quantitatively and qualitatively by visual inspection.</li>
<li class="pidoc_spaced_list_item">Don't use the <a href="../../scripts/BatchPreprocessing/BatchPreprocessing.html" title="../../scripts/BatchPreprocessing/BatchPreprocessing.html">BatchPreprocessing</a> script to integrate your light frames. In most cases, BatchPreprocessing does a fine job for generation of master calibration frames, image calibration and registration. However, integration of light frames is a critical process requiring manual intervention to fine tune pixel rejection and image combination parameters. The integrated output of BatchPreprocessing can be used as a <em>quick preview</em> of the image that can be achieved, but it is not the optimal image by any means, and many times you're quite likely to get a grossly wrong result (e.g., invalid rejection of plane and satellite trails, etc.).</li>
<li class="pidoc_spaced_list_item">Refine your pixel rejection parameters to achieve the highest possible effective noise reduction with appropriate rejection of spurious data (plane and satellite trails, cosmic ray impacts, CCD defective pixels, etc.). We strongly recommend you read an excellent presentation by Jordi Gallego, <sup><a href="#__reference_18__" class="pidoc_referenceTooltip" onmouseover="pidoc_showReferenceToolTip( this );" onmouseout="pidoc_hideReferenceToolTip();" data-tooltip="[Reference 18]<br/>
Jordi Gallego (2010), <a href='http://astrosurf.com/jordigallego/articles/Image_integration_JGallego.ppt' title='http://astrosurf.com/jordigallego/articles/Image_integration_JGallego.ppt'><em>Image integration techniques: Increasing SNR and outlier rejection with PixInsight</em></a>, slides for a presentation given at the <em>VI Seminario de Astrofotograf√≠a de Cielo Profundo</em>, Madrid, November 20th, 2010.">[18]</a></sup> where he describes this task with detailed practical examples and real-world tests. Although this presentation describes an old version of the ImageIntegration tool, the fundamental concepts exposed remain equally valid.</li>
<li class="pidoc_spaced_list_item">Experiment with different <a href="#scale_estimators">scale estimators</a> to discover which ones provide the best results for your data. We suggest you compare the results achieved with the IKSS, MAD and average absolute deviation estimators as a starting point.</li>
<li class="pidoc_spaced_list_item">Unless you have a strong reason to do otherwise, use the noise evaluation <a href="#image_weighting">weighting method</a>. In all of our tests this method consistently leads to the highest SNR improvement in the integrated images.</li>
<li class="pidoc_spaced_list_item">Never use <a href="#median_combination">median combination</a> for production work. As we have explained in this document, median combination will lead to a 20% loss of signal with respect to average combination (or more for small image sets). Always use <a href="#average_combination">average combination</a> and the appropriate <a href="#pixel_rejection">pixel rejection algorithm</a>. Use median combination exclusively as a counter-test to evaluate rejection performance.</li>
<li class="pidoc_spaced_list_item">Never use <a href="#minmax_clipping">min/max rejection</a> for production work. The min/max method rejects a fixed number of samples from each pixel stack without any statistical basis. It will lead to a constant loss of signal proportional to the square root of the number of clipped pixels. While the importance of this loss is inversely proportional to the number of integrated images, better results can <em>always</em> be achieved with more sophisticated rejection algorithms. Use min/max exclusively as a counter-test to evaluate the performance of other algorithms.</li>
<li class="pidoc_spaced_list_item">For integration of master bias and master dark frames, you may want to disable the <a href="#evaluate_noise">Evaluate noise</a> option to accelerate the process, since a quality assessment is normaly not necessary in these cases.</li>
<li class="pidoc_spaced_list_item">Use <a href="#region_of_interest">regions of interest</a> to accelerate repeated tests for the same data set.</li>
<li class="pidoc_spaced_list_item">If you have to integrate images generated by other applications, use <a href="#format_hints">input hints</a> to adapt the alien data to the PixInsight platform. In particular, you probably will have to use the <span class="pidoc_code">&quot;upper-range 65535&quot;</span> input hint for the FITS format. A much better solution is: stop using those applications and calibrate and register your images with PixInsight.</li>
<li class="pidoc_spaced_list_item">If you are using a 32-bit version of PixInsight, you may easily get out-of-memory errors for relatively small data sets. The only good solution to this problem is running a 64-bit version of PixInsight on a 64-bit operating system (preferably FreeBSD or Linux; Mac OS X and Windows also seem to work :)), on a machine with abundant RAM. A workaround for 32-bit systems is decreasing the <a href="#buffer_size">buffer size</a> parameter.</li>
</ul>

</div>

   </div>
</div>

<div class="pidoc_section" id="__references__">
   <h3 class="pidoc_sectionTitle">References</h3>
   <div id="references">
      <p id="__reference_1__"><strong>[1]</strong> Jean-Luc Starck and Fionn Murtagh (1998), <em>Automatic Noise Estimation from the Multiresolution Support</em>, Publications of the Royal Astronomical Society of the Pacific, vol. 110, pp. 193&ndash;199</p>
      <p id="__reference_2__"><strong>[2]</strong> Jean-Luc Starck and Fionn Murtagh (2002), <em>Astronomical Image and Data Analysis</em>, Springer, pp. 36&ndash;39</p>
      <p id="__reference_3__"><strong>[3]</strong> John W. Tukey (1962), <em>The Future of Data Analysis</em>, The Annals of Mathematical Statistics, Vol. 33, No. 1, pp. 17&ndash;19</p>
      <p id="__reference_4__"><strong>[4]</strong> Peter J. Huber and E. Ronchetti (2009), <em>Robust Statistics</em>, 2nd Ed., Wiley</p>
      <p id="__reference_5__"><strong>[5]</strong> William H. Press et al. (2007), <em>Numerical Recipes, The Art of Scientific Computing</em>, 3rd Ed., Cambridge University Press, &sect; 15.7.3, pp. 822&ndash;824</p>
      <p id="__reference_6__"><strong>[6]</strong> Lawrence O'Gorman et al. (2009), <em>Practical Algorithms for Image Analysis</em>, 2nd Ed., Cambridge University Press, &sect; 5.5.2, pp. 214&ndash;215</p>
      <p id="__reference_7__"><strong>[7]</strong> R. A. Shaw, K. Horne (1992), <em>Noise Model-Based Cosmic Ray Rejection for WF/PC Images</em>, Astronomical Data Analysis Software and Systems I, A.S.P. Conference Series, Vol. 25, pp. 311&ndash;315</p>
      <p id="__reference_8__"><strong>[8]</strong> Thomas H. Cormen et al. (2009), <em>Introduction to Algorithms</em>, 3rd Ed., MIT Press, &sect; 9.3, pp. 220&ndash;222</p>
      <p id="__reference_9__"><strong>[9]</strong> Robert Sedgewick, Kevin Wayne (2011), <em>Algorithms</em>, 4th Ed., Addison-Wesley Professional, pp. 345&ndash;347</p>
      <p id="__reference_10__"><strong>[10]</strong> Donald E. Knuth (1973), <em>The Art of Computer Programming, Volume 3: Sorting and Searching</em>, Addison Wesley.</p>
      <p id="__reference_11__"><strong>[11]</strong> W. D. Hillis (1992), <em>Co-evolving parasites improve simulated evolution as an optimization procedure</em>, Langton, C. et al. (Eds.), Artificial Life II. Addison Wesley.</p>
      <p id="__reference_12__"><strong>[12]</strong> Hugues Juill√© (1995), <em>Evolution of Non-Deterministic Incremental Algorithms as a New Approach for Search in State Spaces</em></p>
      <p id="__reference_13__"><strong>[13]</strong> Rand R. Wilcox (2012), <em>Introduction to Robust Estimation and Hypothesis Testing, 3rd Edition</em>, Elsevier Inc., &sect; 3.1.</p>
      <p id="__reference_14__"><strong>[14]</strong> Rand R. Wilcox (2012), <em>Introduction to Robust Estimation and Hypothesis Testing, 3rd Edition</em>, Elsevier Inc., &sect; 3.12.</p>
      <p id="__reference_15__"><strong>[15]</strong> P.J. Rousseeuw and C. Croux (1993), <em>Alternatives to the Median Absolute Deviation</em>, Journal of the American Statistical Association, Vol. 88, pp. 1273&ndash;1283</p>
      <p id="__reference_16__"><strong>[16]</strong> J. W. McKean, R. M. Schrader (1984), <em>A Comparison of Methods for Studentizing the Sample Median</em>, Communications in Statistics &ndash; Simulation and Computation, 13, pp. 751&ndash;773</p>
      <p id="__reference_17__"><strong>[17]</strong> Vicent Peris (2010), <a href="http://pixinsight.com/tutorials/master-frames/en.html" title="http://pixinsight.com/tutorials/master-frames/en.html"><em>Master Calibration Frames: Acquisition and Processing</em></a>, tutorial.</p>
      <p id="__reference_18__"><strong>[18]</strong> Jordi Gallego (2010), <a href="http://astrosurf.com/jordigallego/articles/Image_integration_JGallego.ppt" title="http://astrosurf.com/jordigallego/articles/Image_integration_JGallego.ppt"><em>Image integration techniques: Increasing SNR and outlier rejection with PixInsight</em></a>, slides for a presentation given at the <em>VI Seminario de Astrofotograf√≠a de Cielo Profundo</em>, Madrid, November 20th, 2010.</p>
   </div>
</div>

<div class="pidoc_section" id="__related_tools__">
   <h3 class="pidoc_sectionTitle">Related Tools</h3>
   <div id="related_tools">
<p><a href="../../tools/ImageCalibration/ImageCalibration.html" title="../../tools/ImageCalibration/ImageCalibration.html">ImageCalibration</a>, <a href="../../tools/StarAlignment/StarAlignment.html" title="../../tools/StarAlignment/StarAlignment.html">StarAlignment</a>, <a href="../../tools/HDRComposition/HDRComposition.html" title="../../tools/HDRComposition/HDRComposition.html">HDRComposition</a>, <a href="../../tools/Blink/Blink.html" title="../../tools/Blink/Blink.html">Blink</a></p>
   </div>
</div>

<div class="pidoc_section" id="__related_scripts__">
   <h3 class="pidoc_sectionTitle">Related Scripts</h3>
   <div id="related_scripts">
<p><a href="../../scripts/SubframeSelector/SubframeSelector.html" title="../../scripts/SubframeSelector/SubframeSelector.html">SubframeSelector</a>, <a href="../../scripts/BatchPreprocessing/BatchPreprocessing.html" title="../../scripts/BatchPreprocessing/BatchPreprocessing.html">BatchPreprocessing</a></p>
   </div>
</div>

<hr class="separator"/>

<div id="copyright">
   <p>Copyright &copy; 2011-2013 Pleiades Astrophoto S.L.</p>
</div>

<div id="footer">
   <p>Generated by the PixInsight Documentation Compiler script version 1.6.3 on 2018-12-04 19:24:26 UTC</p>
</div>
<br/>
<br/>

</div> <!-- contents -->

</body>
</html>
